{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SuiteFactory' object has no attribute 'get_expectation_suite'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m context \u001b[38;5;241m=\u001b[39m ge\u001b[38;5;241m.\u001b[39mget_context()\n\u001b[1;32m     29\u001b[0m expectation_suite_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdq_suite\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuites\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_expectation_suite\u001b[49m(expectation_suite_name):\n\u001b[1;32m     32\u001b[0m     context\u001b[38;5;241m.\u001b[39madd_expectation_suite(expectation_suite_name)\n\u001b[1;32m     34\u001b[0m datasource_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdq_csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SuiteFactory' object has no attribute 'get_expectation_suite'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import great_expectations as ge\n",
    "from great_expectations.core.batch import BatchRequest\n",
    "from great_expectations.data_context import FileDataContext\n",
    "import os\n",
    "\n",
    "# Sample dataset\n",
    "csv_path = \"sample.csv\"\n",
    "df = pd.DataFrame({\n",
    "    \"Name\": [\"Alice\", \"Bob\", None, \"David\"],\n",
    "    \"Email\": [\"alice@example.com\", \"bob[at]mail.com\", \"charlie@example.com\", None],\n",
    "    \"Age\": [25, 30, None, 40]\n",
    "})\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "# Task 1: Metric Calculations\n",
    "def calculate_metrics(df):\n",
    "    completeness = df.notnull().mean().mean() * 100\n",
    "    validity = df['Email'].dropna().str.contains(\"@\").mean() * 100\n",
    "    uniqueness = df['Email'].nunique() / len(df) * 100\n",
    "    return completeness, validity, uniqueness\n",
    "\n",
    "# Task 2: Data Quality Score\n",
    "def calculate_quality_score(completeness, validity, uniqueness):\n",
    "    return (completeness + validity + uniqueness) / 3\n",
    "\n",
    "# Task 3: Create Expectations\n",
    "context = ge.get_context()\n",
    "expectation_suite_name = \"dq_suite\"\n",
    "\n",
    "if not context.suites.get_expectation_suite(expectation_suite_name):\n",
    "    context.add_expectation_suite(expectation_suite_name)\n",
    "\n",
    "datasource_name = \"dq_csv\"\n",
    "if datasource_name not in context.list_datasources():\n",
    "    context.sources.add_pandas(datasource_name=datasource_name)\n",
    "\n",
    "validator = context.sources.pandas_default.read_csv(csv_path)\n",
    "validator.expect_column_values_to_not_be_null(\"Name\")\n",
    "validator.expect_column_values_to_not_be_null(\"Email\")\n",
    "validator.expect_column_values_to_match_regex(\"Email\", r\".+@.+\\\\..+\")\n",
    "context.save_expectation_suite(validator.get_expectation_suite(), expectation_suite_name)\n",
    "\n",
    "# Task 4: Validation and Report\n",
    "docs_path = os.path.join(context.root_directory, \"uncommitted\", \"data_docs\")\n",
    "checkpoint = context.add_or_update_checkpoint(\n",
    "    name=\"dq_checkpoint\",\n",
    "    validations=[\n",
    "        {\n",
    "            \"batch_request\": {\n",
    "                \"runtime_parameters\": {\"path\": csv_path},\n",
    "                \"datasource_name\": \"dq_csv\",\n",
    "                \"data_connector_name\": \"default_inferred_data_connector_name\",\n",
    "                \"data_asset_name\": \"sample\",\n",
    "                \"runtime_parameters\": {\"batch_data\": df},\n",
    "                \"batch_identifiers\": {\"default_identifier_name\": \"default_id\"},\n",
    "            },\n",
    "            \"expectation_suite_name\": expectation_suite_name,\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "checkpoint_result = checkpoint.run()\n",
    "context.build_data_docs()\n",
    "print(f\"Report generated at: {docs_path}\")\n",
    "\n",
    "# Task 5: Automate Metric + Quality Score\n",
    "completeness, validity, uniqueness = calculate_metrics(df)\n",
    "score = calculate_quality_score(completeness, validity, uniqueness)\n",
    "print(f\"Data Quality Score: {score:.2f}%\")\n",
    "\n",
    "# Task 6: Automated Cleaning\n",
    "THRESHOLD = 80\n",
    "if score < THRESHOLD:\n",
    "    print(\"Score below threshold. Triggering cleaning script...\")\n",
    "    df['Name'].fillna(\"Unknown\", inplace=True)\n",
    "    df['Email'] = df['Email'].apply(lambda x: x if pd.notnull(x) and '@' in x else 'unknown@example.com')\n",
    "    df['Age'].fillna(df['Age'].mean(), inplace=True)\n",
    "    df.to_csv(\"cleaned_sample.csv\", index=False)\n",
    "    print(\"Cleaned data saved to 'cleaned_sample.csv'\")\n",
    "else:\n",
    "    print(\"Data quality is acceptable. No cleaning necessary.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
