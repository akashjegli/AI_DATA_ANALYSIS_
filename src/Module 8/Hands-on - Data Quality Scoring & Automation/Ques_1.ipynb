{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'great_expectations.dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExpectationSuite\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PandasDataset\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrender\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrenderer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ValidationResultsPageRenderer\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RuntimeBatchRequest\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'great_expectations.dataset'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from great_expectations.core import ExpectationSuite\n",
    "from great_expectations.dataset import PandasDataset\n",
    "from great_expectations.render.renderer import ValidationResultsPageRenderer\n",
    "from great_expectations.core.batch import RuntimeBatchRequest\n",
    "from great_expectations.data_context import BaseDataContext\n",
    "from great_expectations.data_context.types.base import DataContextConfig\n",
    "\n",
    "# Let's create a sample CSV for demonstration\n",
    "def create_sample_data():\n",
    "    \"\"\"Create a sample dataset with Name, Email, Age columns\"\"\"\n",
    "    data = {\n",
    "        'Name': ['John Doe', 'Jane Smith', None, 'Robert Brown', 'Emily White', 'David Green'],\n",
    "        'Email': ['john@example.com', 'jane.smith@company.org', 'invalid-email', None, 'emily@domain.com', 'emily@domain.com'],\n",
    "        'Age': [34, 28, 45, None, 22, 'thirty']\n",
    "    }\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv('sample_data.csv', index=False)\n",
    "    return df\n",
    "\n",
    "# Task 1: Understanding and Defining Data Quality Metrics\n",
    "def calculate_basic_metrics(df):\n",
    "    \"\"\"Calculate basic data quality metrics: completeness, validity, uniqueness\"\"\"\n",
    "    \n",
    "    # Calculate completeness (percentage of non-null values)\n",
    "    completeness = {}\n",
    "    for column in df.columns:\n",
    "        non_null_count = df[column].count()\n",
    "        total_count = len(df)\n",
    "        completeness[column] = (non_null_count / total_count) * 100\n",
    "    \n",
    "    # Calculate validity (% of email fields containing @)\n",
    "    # Only applicable for the Email column\n",
    "    valid_emails = sum(1 for email in df['Email'] if isinstance(email, str) and '@' in email)\n",
    "    email_validity = (valid_emails / len(df)) * 100\n",
    "    \n",
    "    # Calculate uniqueness (count distinct entries in Email column)\n",
    "    unique_emails = df['Email'].nunique()\n",
    "    email_uniqueness = (unique_emails / len(df)) * 100\n",
    "    \n",
    "    # Create a metrics dictionary\n",
    "    metrics = {\n",
    "        'completeness': completeness,\n",
    "        'email_validity': email_validity,\n",
    "        'email_uniqueness': email_uniqueness\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Task 2: Calculating Data Quality Score\n",
    "def calculate_quality_score(metrics):\n",
    "    \"\"\"Aggregate multiple metrics to calculate an overall data quality score\"\"\"\n",
    "    \n",
    "    # Calculate average completeness across all columns\n",
    "    avg_completeness = sum(metrics['completeness'].values()) / len(metrics['completeness'])\n",
    "    \n",
    "    # Use all three metrics for the overall score\n",
    "    overall_score = (avg_completeness + metrics['email_validity'] + metrics['email_uniqueness']) / 3\n",
    "    \n",
    "    return overall_score\n",
    "\n",
    "# Task 3: Creating Expectations for a CSV\n",
    "def create_expectations():\n",
    "    \"\"\"Develop basic data quality expectations using Great Expectations\"\"\"\n",
    "    \n",
    "    # Create an ExpectationSuite\n",
    "    suite = ExpectationSuite(expectation_suite_name=\"sample_data_suite\")\n",
    "    \n",
    "    # Load your data as a PandasDataset\n",
    "    df = pd.read_csv('sample_data.csv')\n",
    "    dataset = PandasDataset(df, expectation_suite=suite)\n",
    "    \n",
    "    # Define expectations for completeness\n",
    "    dataset.expect_column_values_to_not_be_null('Name')\n",
    "    dataset.expect_column_values_to_not_be_null('Email')\n",
    "    dataset.expect_column_values_to_not_be_null('Age')\n",
    "    \n",
    "    # Define expectations for validity\n",
    "    dataset.expect_column_values_to_match_regex('Email', r'.+@.+\\..+')\n",
    "    dataset.expect_column_values_to_be_of_type('Age', 'int64', mostly=0.9)\n",
    "    \n",
    "    # Define expectations for uniqueness\n",
    "    dataset.expect_column_values_to_be_unique('Email')\n",
    "    \n",
    "    # Save the expectation suite to a JSON file\n",
    "    with open('expectation_suite.json', 'w') as f:\n",
    "        json.dump(dataset.get_expectation_suite().to_json_dict(), f, indent=2)\n",
    "    \n",
    "    return dataset.get_expectation_suite()\n",
    "\n",
    "# Task 4: Running and Validating Expectations\n",
    "def validate_data(df, expectation_suite):\n",
    "    \"\"\"Run the created expectations and generate an output report\"\"\"\n",
    "    \n",
    "    # Convert the DataFrame to a PandasDataset with the expectation suite\n",
    "    dataset = PandasDataset(df, expectation_suite=expectation_suite)\n",
    "    \n",
    "    # Validate the data against the expectations\n",
    "    validation_result = dataset.validate()\n",
    "    \n",
    "    # Print a summary of the validation results\n",
    "    print(f\"Validation successful: {validation_result.success}\")\n",
    "    print(f\"Total expectations: {len(validation_result.results)}\")\n",
    "    print(f\"Passed expectations: {sum(1 for result in validation_result.results if result.success)}\")\n",
    "    \n",
    "    # Create a simple HTML report\n",
    "    with open('validation_report.html', 'w') as f:\n",
    "        f.write(\"<html><body>\")\n",
    "        f.write(\"<h1>Data Validation Report</h1>\")\n",
    "        f.write(f\"<p>Validation successful: {validation_result.success}</p>\")\n",
    "        f.write(\"<h2>Results</h2>\")\n",
    "        f.write(\"<table border='1'>\")\n",
    "        f.write(\"<tr><th>Expectation</th><th>Success</th><th>Details</th></tr>\")\n",
    "        \n",
    "        for result in validation_result.results:\n",
    "            expectation = result.expectation_config\n",
    "            f.write(\"<tr>\")\n",
    "            f.write(f\"<td>{expectation.expectation_type}</td>\")\n",
    "            f.write(f\"<td>{'✅' if result.success else '❌'}</td>\")\n",
    "            \n",
    "            # Add some details about the expectation\n",
    "            details = f\"Column: {expectation.kwargs.get('column', 'N/A')}\"\n",
    "            if 'mostly' in expectation.kwargs:\n",
    "                details += f\", Threshold: {expectation.kwargs['mostly'] * 100}%\"\n",
    "            f.write(f\"<td>{details}</td>\")\n",
    "            \n",
    "            f.write(\"</tr>\")\n",
    "        \n",
    "        f.write(\"</table>\")\n",
    "        f.write(\"</body></html>\")\n",
    "    \n",
    "    return validation_result\n",
    "\n",
    "# Task 5: Automating Data Quality Score Calculation\n",
    "def automate_quality_score(file_path):\n",
    "    \"\"\"Automate the data quality score calculation\"\"\"\n",
    "    \n",
    "    # Load the data\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Calculate basic metrics\n",
    "    metrics = calculate_basic_metrics(df)\n",
    "    \n",
    "    # Calculate the quality score\n",
    "    score = calculate_quality_score(metrics)\n",
    "    \n",
    "    # Load the expectation suite\n",
    "    with open('expectation_suite.json', 'r') as f:\n",
    "        expectation_suite_dict = json.load(f)\n",
    "    \n",
    "    # Convert dict to ExpectationSuite\n",
    "    expectation_suite = ExpectationSuite(\n",
    "        expectation_suite_name=expectation_suite_dict[\"expectation_suite_name\"],\n",
    "        expectations=expectation_suite_dict[\"expectations\"]\n",
    "    )\n",
    "    \n",
    "    # Validate the data against the expectations\n",
    "    validation_result = validate_data(df, expectation_suite)\n",
    "    \n",
    "    # Return the quality score and validation result\n",
    "    return score, validation_result\n",
    "\n",
    "# Task 6: Leveraging Data Quality Metrics for Automated Data Cleaning\n",
    "def clean_data(df, validation_result):\n",
    "    \"\"\"Clean data based on validation results\"\"\"\n",
    "    \n",
    "    cleaned_df = df.copy()\n",
    "    \n",
    "    # Track the number of rows and values cleaned\n",
    "    cleaning_stats = {\n",
    "        'rows_affected': 0,\n",
    "        'cells_cleaned': 0,\n",
    "        'cleaning_actions': []\n",
    "    }\n",
    "    \n",
    "    # Identify failed expectations\n",
    "    failed_expectations = [result.expectation_config for result in validation_result.results if not result.success]\n",
    "    \n",
    "    for expectation in failed_expectations:\n",
    "        expectation_type = expectation.expectation_type\n",
    "        column = expectation.kwargs.get('column')\n",
    "        \n",
    "        # Clean null values\n",
    "        if expectation_type == 'expect_column_values_to_not_be_null' and column:\n",
    "            # For demo purposes, we'll use simple imputation strategies\n",
    "            if column == 'Name':\n",
    "                # Fill missing names with placeholder\n",
    "                missing_mask = cleaned_df[column].isna()\n",
    "                cleaned_df.loc[missing_mask, column] = 'Unknown User'\n",
    "                cleaning_stats['cells_cleaned'] += missing_mask.sum()\n",
    "                cleaning_stats['rows_affected'] += missing_mask.sum()\n",
    "                cleaning_stats['cleaning_actions'].append(f\"Filled {missing_mask.sum()} missing values in {column}\")\n",
    "            \n",
    "            elif column == 'Email':\n",
    "                # Generate placeholder emails based on name\n",
    "                missing_mask = cleaned_df[column].isna()\n",
    "                for idx in cleaned_df[missing_mask].index:\n",
    "                    name = cleaned_df.loc[idx, 'Name']\n",
    "                    if pd.isna(name) or name == 'Unknown User':\n",
    "                        email = f\"user{idx}@placeholder.com\"\n",
    "                    else:\n",
    "                        # Create an email from the name\n",
    "                        name_parts = name.lower().split()\n",
    "                        email = f\"{name_parts[0]}.{name_parts[-1]}@placeholder.com\"\n",
    "                    \n",
    "                    cleaned_df.loc[idx, column] = email\n",
    "                    cleaning_stats['cells_cleaned'] += 1\n",
    "                    cleaning_stats['rows_affected'] += 1\n",
    "                \n",
    "                cleaning_stats['cleaning_actions'].append(f\"Generated {missing_mask.sum()} missing emails\")\n",
    "            \n",
    "            elif column == 'Age':\n",
    "                # Fill missing ages with the median age\n",
    "                median_age = cleaned_df[column].median()\n",
    "                if pd.isna(median_age):  # If all values are null\n",
    "                    median_age = 30  # Default value\n",
    "                \n",
    "                missing_mask = cleaned_df[column].isna()\n",
    "                cleaned_df.loc[missing_mask, column] = median_age\n",
    "                cleaning_stats['cells_cleaned'] += missing_mask.sum()\n",
    "                cleaning_stats['rows_affected'] += missing_mask.sum()\n",
    "                cleaning_stats['cleaning_actions'].append(f\"Filled {missing_mask.sum()} missing ages with median: {median_age}\")\n",
    "        \n",
    "        # Fix invalid emails\n",
    "        if expectation_type == 'expect_column_values_to_match_regex' and column == 'Email':\n",
    "            # Find emails that don't match the regex pattern\n",
    "            invalid_mask = ~cleaned_df[column].str.contains('@', na=False)\n",
    "            for idx in cleaned_df[invalid_mask].index:\n",
    "                name = cleaned_df.loc[idx, 'Name']\n",
    "                if pd.isna(name) or name == 'Unknown User':\n",
    "                    email = f\"user{idx}@corrected.com\"\n",
    "                else:\n",
    "                    # Create an email from the name\n",
    "                    name_parts = name.lower().split()\n",
    "                    email = f\"{name_parts[0]}.{name_parts[-1]}@corrected.com\"\n",
    "                \n",
    "                cleaned_df.loc[idx, column] = email\n",
    "                cleaning_stats['cells_cleaned'] += 1\n",
    "                cleaning_stats['rows_affected'] += 1\n",
    "            \n",
    "            cleaning_stats['cleaning_actions'].append(f\"Corrected {invalid_mask.sum()} invalid email formats\")\n",
    "        \n",
    "        # Fix data types\n",
    "        if expectation_type == 'expect_column_values_to_be_of_type' and column == 'Age':\n",
    "            # Convert non-numeric ages to numeric\n",
    "            for idx in cleaned_df.index:\n",
    "                age_value = cleaned_df.loc[idx, column]\n",
    "                if not pd.isna(age_value) and not isinstance(age_value, (int, float)):\n",
    "                    try:\n",
    "                        # Try to convert string numbers to int\n",
    "                        cleaned_df.loc[idx, column] = int(age_value)\n",
    "                    except ValueError:\n",
    "                        # For text like \"thirty\", use a default value\n",
    "                        cleaned_df.loc[idx, column] = 30\n",
    "                        cleaning_stats['cells_cleaned'] += 1\n",
    "                        cleaning_stats['rows_affected'] += 1\n",
    "            \n",
    "            cleaning_stats['cleaning_actions'].append(\"Converted non-numeric ages to numeric values\")\n",
    "        \n",
    "        # Fix duplicate emails\n",
    "        if expectation_type == 'expect_column_values_to_be_unique' and column == 'Email':\n",
    "            # Find duplicate emails\n",
    "            duplicates = cleaned_df[cleaned_df.duplicated(subset=[column], keep='first')][column].tolist()\n",
    "            \n",
    "            if duplicates:\n",
    "                # Make duplicated emails unique by adding a counter\n",
    "                counter = {}\n",
    "                for idx in cleaned_df.index:\n",
    "                    email = cleaned_df.loc[idx, column]\n",
    "                    \n",
    "                    if email in duplicates:\n",
    "                        if email not in counter:\n",
    "                            counter[email] = 1\n",
    "                        else:\n",
    "                            counter[email] += 1\n",
    "                            # Modify the duplicated email to make it unique\n",
    "                            base_email = email.split('@')\n",
    "                            new_email = f\"{base_email[0]}+{counter[email]}@{base_email[1]}\"\n",
    "                            cleaned_df.loc[idx, column] = new_email\n",
    "                            cleaning_stats['cells_cleaned'] += 1\n",
    "                            cleaning_stats['rows_affected'] += 1\n",
    "                \n",
    "                cleaning_stats['cleaning_actions'].append(f\"Made {sum(counter.values()) - len(counter)} duplicate emails unique\")\n",
    "    \n",
    "    return cleaned_df, cleaning_stats\n",
    "\n",
    "# Main function to execute the full data quality workflow\n",
    "def main():\n",
    "    \"\"\"Execute the full data quality workflow\"\"\"\n",
    "    \n",
    "    # Create sample data\n",
    "    print(\"Creating sample data...\")\n",
    "    df = create_sample_data()\n",
    "    print(df)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "    \n",
    "    # Task 1: Calculate basic metrics\n",
    "    print(\"Calculating basic data quality metrics...\")\n",
    "    metrics = calculate_basic_metrics(df)\n",
    "    print(f\"Completeness: {metrics['completeness']}\")\n",
    "    print(f\"Email Validity: {metrics['email_validity']:.2f}%\")\n",
    "    print(f\"Email Uniqueness: {metrics['email_uniqueness']:.2f}%\")\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "    \n",
    "    # Task 2: Calculate quality score\n",
    "    print(\"Calculating overall data quality score...\")\n",
    "    score = calculate_quality_score(metrics)\n",
    "    print(f\"Overall Data Quality Score: {score:.2f}%\")\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "    \n",
    "    # Task 3: Create expectations\n",
    "    print(\"Creating expectations using Great Expectations...\")\n",
    "    expectation_suite = create_expectations()\n",
    "    print(\"Expectations created and saved to 'expectation_suite.json'\")\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "    \n",
    "    # Task 4: Validate data\n",
    "    print(\"Validating data against expectations...\")\n",
    "    validation_result = validate_data(df, expectation_suite)\n",
    "    print(\"Validation report saved to 'validation_report.html'\")\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "    \n",
    "    # Task 5 & 6: Automated cleaning based on quality metrics\n",
    "    print(\"Checking if data cleaning is needed...\")\n",
    "    # Define a quality threshold\n",
    "    QUALITY_THRESHOLD = 80.0\n",
    "    \n",
    "    if score < QUALITY_THRESHOLD:\n",
    "        print(f\"Data quality score ({score:.2f}%) is below threshold ({QUALITY_THRESHOLD}%).\")\n",
    "        print(\"Initiating automated data cleaning...\")\n",
    "        \n",
    "        # Clean the data\n",
    "        cleaned_df, cleaning_stats = clean_data(df, validation_result)\n",
    "        \n",
    "        # Save the cleaned data\n",
    "        cleaned_df.to_csv('cleaned_data.csv', index=False)\n",
    "        \n",
    "        # Print cleaning statistics\n",
    "        print(\"\\nCleaning Statistics:\")\n",
    "        print(f\"Rows affected: {cleaning_stats['rows_affected']}\")\n",
    "        print(f\"Cells cleaned: {cleaning_stats['cells_cleaned']}\")\n",
    "        print(\"\\nCleaning actions performed:\")\n",
    "        for action in cleaning_stats['cleaning_actions']:\n",
    "            print(f\"- {action}\")\n",
    "        \n",
    "        # Recalculate metrics and score for the cleaned data\n",
    "        new_metrics = calculate_basic_metrics(cleaned_df)\n",
    "        new_score = calculate_quality_score(new_metrics)\n",
    "        \n",
    "        print(\"\\nNew Data Quality Score after cleaning: {:.2f}%\".format(new_score))\n",
    "        print(\"\\nCleaned data saved to 'cleaned_data.csv'\")\n",
    "        \n",
    "        # Display the cleaned data\n",
    "        print(\"\\nCleaned Data:\")\n",
    "        print(cleaned_df)\n",
    "    else:\n",
    "        print(f\"Data quality score ({score:.2f}%) is above threshold ({QUALITY_THRESHOLD}%).\")\n",
    "        print(\"No cleaning needed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
