{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'great_expectations.dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, List, Tuple, Any, Optional\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExpectationSuite\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PandasDataset\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01munittest\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01munittest\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmock\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m patch, MagicMock\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'great_expectations.dataset'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "from great_expectations.core import ExpectationSuite\n",
    "from great_expectations.dataset import PandasDataset\n",
    "import unittest\n",
    "from unittest.mock import patch, MagicMock\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DataQualityError(Exception):\n",
    "    \"\"\"Custom exception for data quality related errors\"\"\"\n",
    "    pass\n",
    "\n",
    "# Task 1: Understanding and Defining Data Quality Metrics\n",
    "def calculate_basic_metrics(df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Calculate basic data quality metrics: completeness, validity, uniqueness\n",
    "    \n",
    "    Args:\n",
    "        df: Input DataFrame to analyze\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing calculated metrics\n",
    "        \n",
    "    Raises:\n",
    "        DataQualityError: If metrics calculation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        metrics = {}\n",
    "        \n",
    "        # Calculate completeness (percentage of non-null values) - vectorized\n",
    "        completeness = {}\n",
    "        total_count = len(df)\n",
    "        # Ensure we don't divide by zero\n",
    "        if total_count == 0:\n",
    "            raise DataQualityError(\"Cannot calculate metrics on empty DataFrame\")\n",
    "            \n",
    "        for column in df.columns:\n",
    "            non_null_count = df[column].count()\n",
    "            completeness[column] = (non_null_count / total_count) * 100\n",
    "        \n",
    "        # Calculate validity (% of email fields containing @) - vectorized\n",
    "        # Only applicable for the Email column\n",
    "        if 'Email' in df.columns:\n",
    "            # Using vectorized operations instead of a loop\n",
    "            valid_emails = df['Email'].str.contains('@', na=False).sum()\n",
    "            email_validity = (valid_emails / total_count) * 100\n",
    "        else:\n",
    "            email_validity = 0\n",
    "            logger.warning(\"Email column not found in DataFrame. Setting email_validity to 0.\")\n",
    "        \n",
    "        # Calculate uniqueness (count distinct entries in Email column) - vectorized\n",
    "        if 'Email' in df.columns:\n",
    "            unique_emails = df['Email'].nunique(dropna=False)  # Include NaN in count\n",
    "            email_uniqueness = (unique_emails / total_count) * 100\n",
    "        else:\n",
    "            email_uniqueness = 0\n",
    "            logger.warning(\"Email column not found in DataFrame. Setting email_uniqueness to 0.\")\n",
    "        \n",
    "        # Create a metrics dictionary\n",
    "        metrics = {\n",
    "            'completeness': completeness,\n",
    "            'email_validity': email_validity,\n",
    "            'email_uniqueness': email_uniqueness\n",
    "        }\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating metrics: {str(e)}\")\n",
    "        raise DataQualityError(f\"Failed to calculate data quality metrics: {str(e)}\")\n",
    "\n",
    "# Task 2: Calculating Data Quality Score\n",
    "def calculate_quality_score(metrics: Dict[str, Any]) -> float:\n",
    "    \"\"\"\n",
    "    Aggregate multiple metrics to calculate an overall data quality score\n",
    "    \n",
    "    Args:\n",
    "        metrics: Dictionary containing data quality metrics\n",
    "        \n",
    "    Returns:\n",
    "        Overall data quality score as a percentage\n",
    "        \n",
    "    Raises:\n",
    "        DataQualityError: If score calculation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Calculate average completeness across all columns\n",
    "        if not metrics['completeness']:\n",
    "            raise DataQualityError(\"Completeness metrics are missing\")\n",
    "            \n",
    "        avg_completeness = sum(metrics['completeness'].values()) / len(metrics['completeness'])\n",
    "        \n",
    "        # Use all three metrics for the overall score\n",
    "        overall_score = (avg_completeness + metrics['email_validity'] + metrics['email_uniqueness']) / 3\n",
    "        \n",
    "        return overall_score\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error calculating quality score: {str(e)}\")\n",
    "        raise DataQualityError(f\"Failed to calculate quality score: {str(e)}\")\n",
    "\n",
    "# Function to load or create sample data\n",
    "def load_or_create_data(file_path: str = 'sample_data.csv', create_if_missing: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load data from CSV or create sample data if file doesn't exist\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the CSV file\n",
    "        create_if_missing: Whether to create sample data if file doesn't exist\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame containing the data\n",
    "        \n",
    "    Raises:\n",
    "        FileNotFoundError: If file doesn't exist and create_if_missing is False\n",
    "        DataQualityError: If data creation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if os.path.exists(file_path):\n",
    "            logger.info(f\"Loading data from {file_path}\")\n",
    "            return pd.read_csv(file_path)\n",
    "        elif create_if_missing:\n",
    "            logger.info(f\"File {file_path} not found. Creating sample data.\")\n",
    "            # Create sample data\n",
    "            data = {\n",
    "                'Name': ['John Doe', 'Jane Smith', None, 'Robert Brown', 'Emily White', 'David Green'],\n",
    "                'Email': ['john@example.com', 'jane.smith@company.org', 'invalid-email', None, 'emily@domain.com', 'emily@domain.com'],\n",
    "                'Age': [34, 28, 45, None, 22, 'thirty']\n",
    "            }\n",
    "            df = pd.DataFrame(data)\n",
    "            \n",
    "            # Save to CSV\n",
    "            try:\n",
    "                df.to_csv(file_path, index=False)\n",
    "                logger.info(f\"Sample data saved to {file_path}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to save sample data to {file_path}: {str(e)}\")\n",
    "                \n",
    "            return df\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"File {file_path} not found\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        if isinstance(e, FileNotFoundError):\n",
    "            raise\n",
    "        logger.error(f\"Error loading or creating data: {str(e)}\")\n",
    "        raise DataQualityError(f\"Failed to load or create data: {str(e)}\")\n",
    "\n",
    "# Task 3: Creating Expectations for a CSV\n",
    "def create_expectations(df: Optional[pd.DataFrame] = None, \n",
    "                       file_path: str = 'sample_data.csv',\n",
    "                       suite_name: str = \"sample_data_suite\",\n",
    "                       output_path: str = 'expectation_suite.json') -> ExpectationSuite:\n",
    "    \"\"\"\n",
    "    Develop basic data quality expectations using Great Expectations\n",
    "    \n",
    "    Args:\n",
    "        df: Optional DataFrame to use (loads from file_path if None)\n",
    "        file_path: Path to the CSV file (used if df is None)\n",
    "        suite_name: Name for the expectation suite\n",
    "        output_path: Where to save the expectation suite JSON\n",
    "        \n",
    "    Returns:\n",
    "        ExpectationSuite object\n",
    "        \n",
    "    Raises:\n",
    "        DataQualityError: If expectations creation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create an ExpectationSuite\n",
    "        suite = ExpectationSuite(expectation_suite_name=suite_name)\n",
    "        \n",
    "        # Load data if not provided\n",
    "        if df is None:\n",
    "            df = load_or_create_data(file_path)\n",
    "        \n",
    "        # Create a PandasDataset with the expectation suite\n",
    "        dataset = PandasDataset(df, expectation_suite=suite)\n",
    "        \n",
    "        # Define expectations for completeness\n",
    "        if 'Name' in df.columns:\n",
    "            dataset.expect_column_values_to_not_be_null('Name')\n",
    "        if 'Email' in df.columns:\n",
    "            dataset.expect_column_values_to_not_be_null('Email')\n",
    "        if 'Age' in df.columns:\n",
    "            dataset.expect_column_values_to_not_be_null('Age')\n",
    "        \n",
    "        # Define expectations for validity\n",
    "        if 'Email' in df.columns:\n",
    "            dataset.expect_column_values_to_match_regex('Email', r'.+@.+\\..+')\n",
    "        if 'Age' in df.columns:\n",
    "            dataset.expect_column_values_to_be_of_type('Age', 'int64', mostly=0.9)\n",
    "        \n",
    "        # Define expectations for uniqueness\n",
    "        if 'Email' in df.columns:\n",
    "            dataset.expect_column_values_to_be_unique('Email')\n",
    "        \n",
    "        # Save the expectation suite to a JSON file\n",
    "        try:\n",
    "            with open(output_path, 'w') as f:\n",
    "                json.dump(dataset.get_expectation_suite().to_json_dict(), f, indent=2)\n",
    "            logger.info(f\"Expectation suite saved to {output_path}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to save expectation suite to {output_path}: {str(e)}\")\n",
    "        \n",
    "        return dataset.get_expectation_suite()\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating expectations: {str(e)}\")\n",
    "        raise DataQualityError(f\"Failed to create expectations: {str(e)}\")\n",
    "\n",
    "# Task 4: Running and Validating Expectations\n",
    "def validate_data(df: pd.DataFrame, \n",
    "                 expectation_suite: ExpectationSuite,\n",
    "                 output_path: str = 'validation_report.html') -> dict:\n",
    "    \"\"\"\n",
    "    Run the created expectations and generate an output report\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to validate\n",
    "        expectation_suite: ExpectationSuite to validate against\n",
    "        output_path: Where to save the HTML report\n",
    "        \n",
    "    Returns:\n",
    "        Validation result dictionary\n",
    "        \n",
    "    Raises:\n",
    "        DataQualityError: If validation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert the DataFrame to a PandasDataset with the expectation suite\n",
    "        dataset = PandasDataset(df, expectation_suite=expectation_suite)\n",
    "        \n",
    "        # Validate the data against the expectations\n",
    "        validation_result = dataset.validate()\n",
    "        \n",
    "        # Print a summary of the validation results\n",
    "        logger.info(f\"Validation successful: {validation_result.success}\")\n",
    "        logger.info(f\"Total expectations: {len(validation_result.results)}\")\n",
    "        logger.info(f\"Passed expectations: {sum(1 for result in validation_result.results if result.success)}\")\n",
    "        \n",
    "        # Create a simple HTML report\n",
    "        try:\n",
    "            with open(output_path, 'w') as f:\n",
    "                f.write(\"<html><body>\")\n",
    "                f.write(\"<h1>Data Validation Report</h1>\")\n",
    "                f.write(f\"<p>Validation successful: {validation_result.success}</p>\")\n",
    "                f.write(\"<h2>Results</h2>\")\n",
    "                f.write(\"<table border='1'>\")\n",
    "                f.write(\"<tr><th>Expectation</th><th>Success</th><th>Details</th></tr>\")\n",
    "                \n",
    "                for result in validation_result.results:\n",
    "                    expectation = result.expectation_config\n",
    "                    f.write(\"<tr>\")\n",
    "                    f.write(f\"<td>{expectation.expectation_type}</td>\")\n",
    "                    f.write(f\"<td>{'✅' if result.success else '❌'}</td>\")\n",
    "                    \n",
    "                    # Add some details about the expectation\n",
    "                    details = f\"Column: {expectation.kwargs.get('column', 'N/A')}\"\n",
    "                    if 'mostly' in expectation.kwargs:\n",
    "                        details += f\", Threshold: {expectation.kwargs['mostly'] * 100}%\"\n",
    "                    f.write(f\"<td>{details}</td>\")\n",
    "                    \n",
    "                    f.write(\"</tr>\")\n",
    "                \n",
    "                f.write(\"</table>\")\n",
    "                f.write(\"</body></html>\")\n",
    "            logger.info(f\"Validation report saved to {output_path}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to save validation report to {output_path}: {str(e)}\")\n",
    "        \n",
    "        return validation_result\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error validating data: {str(e)}\")\n",
    "        raise DataQualityError(f\"Failed to validate data: {str(e)}\")\n",
    "\n",
    "# Task 5: Automating Data Quality Score Calculation\n",
    "def automate_quality_score(file_path: str, expectation_suite_path: str = 'expectation_suite.json') -> Tuple[float, dict]:\n",
    "    \"\"\"\n",
    "    Automate the data quality score calculation with error handling\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the CSV file\n",
    "        expectation_suite_path: Path to the expectation suite JSON\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (quality_score, validation_result)\n",
    "        \n",
    "    Raises:\n",
    "        DataQualityError: If automation fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load the data\n",
    "        df = load_or_create_data(file_path, create_if_missing=False)\n",
    "        \n",
    "        # Calculate basic metrics\n",
    "        metrics = calculate_basic_metrics(df)\n",
    "        \n",
    "        # Calculate the quality score\n",
    "        score = calculate_quality_score(metrics)\n",
    "        \n",
    "        # Load the expectation suite\n",
    "        try:\n",
    "            with open(expectation_suite_path, 'r') as f:\n",
    "                expectation_suite_dict = json.load(f)\n",
    "                \n",
    "            # Convert dict to ExpectationSuite\n",
    "            expectation_suite = ExpectationSuite(\n",
    "                expectation_suite_name=expectation_suite_dict[\"expectation_suite_name\"],\n",
    "                expectations=expectation_suite_dict[\"expectations\"]\n",
    "            )\n",
    "        except FileNotFoundError:\n",
    "            logger.warning(f\"Expectation suite not found at {expectation_suite_path}. Creating new expectations.\")\n",
    "            expectation_suite = create_expectations(df)\n",
    "        \n",
    "        # Validate the data against the expectations\n",
    "        validation_result = validate_data(df, expectation_suite)\n",
    "        \n",
    "        # Return the quality score and validation result\n",
    "        return score, validation_result\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error automating quality score: {str(e)}\")\n",
    "        raise DataQualityError(f\"Failed to automate quality score: {str(e)}\")\n",
    "\n",
    "# Task 6: Leveraging Data Quality Metrics for Automated Data Cleaning\n",
    "def clean_data(df: pd.DataFrame, validation_result: dict) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Clean data based on validation results using vectorized operations where possible\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to clean\n",
    "        validation_result: Validation result from Great Expectations\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (cleaned_dataframe, cleaning_stats)\n",
    "        \n",
    "    Raises:\n",
    "        DataQualityError: If cleaning fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cleaned_df = df.copy()\n",
    "        \n",
    "        # Track the number of rows and values cleaned\n",
    "        cleaning_stats = {\n",
    "            'rows_affected': 0,\n",
    "            'cells_cleaned': 0,\n",
    "            'cleaning_actions': []\n",
    "        }\n",
    "        \n",
    "        # Identify failed expectations\n",
    "        failed_expectations = [result.expectation_config for result in validation_result.results if not result.success]\n",
    "        \n",
    "        # --- Handle missing values (expectation: expect_column_values_to_not_be_null) ---\n",
    "        for expectation in [e for e in failed_expectations if e.expectation_type == 'expect_column_values_to_not_be_null']:\n",
    "            column = expectation.kwargs.get('column')\n",
    "            if not column or column not in cleaned_df.columns:\n",
    "                continue\n",
    "                \n",
    "            missing_mask = cleaned_df[column].isna()\n",
    "            missing_count = missing_mask.sum()\n",
    "            \n",
    "            if missing_count == 0:\n",
    "                continue\n",
    "                \n",
    "            if column == 'Name':\n",
    "                # Fill missing names with placeholder - vectorized\n",
    "                cleaned_df.loc[missing_mask, column] = 'Unknown User'\n",
    "                cleaning_stats['cells_cleaned'] += missing_count\n",
    "                cleaning_stats['rows_affected'] += missing_count\n",
    "                cleaning_stats['cleaning_actions'].append(f\"Filled {missing_count} missing values in {column}\")\n",
    "            \n",
    "            elif column == 'Email':\n",
    "                # For email generation, we still need some iteration\n",
    "                # But we'll use vectorized operations where possible\n",
    "                for idx in cleaned_df[missing_mask].index:\n",
    "                    name = cleaned_df.loc[idx, 'Name']\n",
    "                    if pd.isna(name) or name == 'Unknown User':\n",
    "                        email = f\"user{idx}@placeholder.com\"\n",
    "                    else:\n",
    "                        # Create an email from the name\n",
    "                        name_parts = name.lower().split()\n",
    "                        email = f\"{name_parts[0]}.{name_parts[-1]}@placeholder.com\"\n",
    "                    \n",
    "                    cleaned_df.loc[idx, column] = email\n",
    "                \n",
    "                cleaning_stats['cells_cleaned'] += missing_count\n",
    "                cleaning_stats['rows_affected'] += missing_count\n",
    "                cleaning_stats['cleaning_actions'].append(f\"Generated {missing_count} missing emails\")\n",
    "            \n",
    "            elif column == 'Age':\n",
    "                # Fill missing ages with the median age - vectorized\n",
    "                valid_ages = pd.to_numeric(cleaned_df[column], errors='coerce')\n",
    "                median_age = valid_ages.median()\n",
    "                \n",
    "                if pd.isna(median_age):  # If all values are null or non-numeric\n",
    "                    median_age = 30  # Default value\n",
    "                \n",
    "                cleaned_df.loc[missing_mask, column] = median_age\n",
    "                cleaning_stats['cells_cleaned'] += missing_count\n",
    "                cleaning_stats['rows_affected'] += missing_count\n",
    "                cleaning_stats['cleaning_actions'].append(f\"Filled {missing_count} missing ages with median: {median_age}\")\n",
    "        \n",
    "        # --- Handle invalid emails (expectation: expect_column_values_to_match_regex) ---\n",
    "        for expectation in [e for e in failed_expectations if e.expectation_type == 'expect_column_values_to_match_regex' and e.kwargs.get('column') == 'Email']:\n",
    "            if 'Email' not in cleaned_df.columns:\n",
    "                continue\n",
    "                \n",
    "            # Find emails that don't match the regex pattern - vectorized\n",
    "            invalid_mask = ~cleaned_df['Email'].str.contains('@', na=False)\n",
    "            invalid_count = invalid_mask.sum()\n",
    "            \n",
    "            if invalid_count == 0:\n",
    "                continue\n",
    "            \n",
    "            # For correcting emails, we still need some iteration\n",
    "            for idx in cleaned_df[invalid_mask].index:\n",
    "                name = cleaned_df.loc[idx, 'Name']\n",
    "                if pd.isna(name) or name == 'Unknown User':\n",
    "                    email = f\"user{idx}@corrected.com\"\n",
    "                else:\n",
    "                    # Create an email from the name\n",
    "                    name_parts = name.lower().split()\n",
    "                    email = f\"{name_parts[0]}.{name_parts[-1]}@corrected.com\"\n",
    "                \n",
    "                cleaned_df.loc[idx, 'Email'] = email\n",
    "            \n",
    "            cleaning_stats['cells_cleaned'] += invalid_count\n",
    "            cleaning_stats['rows_affected'] += invalid_count\n",
    "            cleaning_stats['cleaning_actions'].append(f\"Corrected {invalid_count} invalid email formats\")\n",
    "        \n",
    "        # --- Handle non-numeric ages (expectation: expect_column_values_to_be_of_type) ---\n",
    "        for expectation in [e for e in failed_expectations if e.expectation_type == 'expect_column_values_to_be_of_type' and e.kwargs.get('column') == 'Age']:\n",
    "            if 'Age' not in cleaned_df.columns:\n",
    "                continue\n",
    "                \n",
    "            # Vectorized conversion of Age column with pd.to_numeric\n",
    "            original_values = cleaned_df['Age'].copy()\n",
    "            cleaned_df['Age'] = pd.to_numeric(cleaned_df['Age'], errors='coerce')\n",
    "            \n",
    "            # Fill any new NaN values (from coercion) with default\n",
    "            new_nulls_mask = cleaned_df['Age'].isna() & ~original_values.isna()\n",
    "            new_nulls_count = new_nulls_mask.sum()\n",
    "            \n",
    "            if new_nulls_count > 0:\n",
    "                cleaned_df.loc[new_nulls_mask, 'Age'] = 30\n",
    "                cleaning_stats['cells_cleaned'] += new_nulls_count\n",
    "                cleaning_stats['rows_affected'] += new_nulls_count\n",
    "                cleaning_stats['cleaning_actions'].append(f\"Converted {new_nulls_count} non-numeric ages to default value (30)\")\n",
    "        \n",
    "        # --- Handle duplicate emails (expectation: expect_column_values_to_be_unique) ---\n",
    "        for expectation in [e for e in failed_expectations if e.expectation_type == 'expect_column_values_to_be_unique' and e.kwargs.get('column') == 'Email']:\n",
    "            if 'Email' not in cleaned_df.columns:\n",
    "                continue\n",
    "                \n",
    "            # Find duplicate emails - vectorized identification\n",
    "            duplicate_mask = cleaned_df.duplicated(subset=['Email'], keep='first')\n",
    "            duplicate_count = duplicate_count = duplicate_mask.sum()\n",
    "            \n",
    "            if duplicate_count == 0:\n",
    "                continue\n",
    "                \n",
    "            # Extract duplicated values for processing\n",
    "            duplicate_emails = cleaned_df.loc[duplicate_mask, 'Email'].values\n",
    "            \n",
    "            # For making duplicates unique, we need some iteration\n",
    "            dup_counter = {}\n",
    "            for idx in cleaned_df[duplicate_mask].index:\n",
    "                email = cleaned_df.loc[idx, 'Email']\n",
    "                \n",
    "                if email not in dup_counter:\n",
    "                    dup_counter[email] = 1\n",
    "                else:\n",
    "                    dup_counter[email] += 1\n",
    "                    # Modify the duplicated email to make it unique\n",
    "                    if isinstance(email, str) and '@' in email:\n",
    "                        base_email = email.split('@')\n",
    "                        new_email = f\"{base_email[0]}+{dup_counter[email]}@{base_email[1]}\"\n",
    "                        cleaned_df.loc[idx, 'Email'] = new_email\n",
    "            \n",
    "            modified_count = sum(v - 1 for v in dup_counter.values() if v > 0)\n",
    "            if modified_count > 0:\n",
    "                cleaning_stats['cells_cleaned'] += modified_count\n",
    "                cleaning_stats['rows_affected'] += modified_count\n",
    "                cleaning_stats['cleaning_actions'].append(f\"Made {modified_count} duplicate emails unique\")\n",
    "        \n",
    "        return cleaned_df, cleaning_stats\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error cleaning data: {str(e)}\")\n",
    "        raise DataQualityError(f\"Failed to clean data: {str(e)}\")\n",
    "\n",
    "# Function to check data quality and trigger cleaning if needed\n",
    "def check_quality_and_clean(df: pd.DataFrame, \n",
    "                          quality_score: float, \n",
    "                          validation_result: dict,\n",
    "                          threshold: float = 80.0) -> Tuple[pd.DataFrame, bool, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Check data quality against threshold and clean if needed\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to check and potentially clean\n",
    "        quality_score: The calculated quality score\n",
    "        validation_result: Validation result from Great Expectations\n",
    "        threshold: Quality threshold to trigger cleaning\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (dataframe, was_cleaned, cleaning_stats)\n",
    "        \n",
    "    Raises:\n",
    "        DataQualityError: If quality check or cleaning fails\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cleaning_stats = {\n",
    "            'rows_affected': 0,\n",
    "            'cells_cleaned': 0,\n",
    "            'cleaning_actions': []\n",
    "        }\n",
    "        \n",
    "        if quality_score < threshold:\n",
    "            logger.info(f\"Data quality score ({quality_score:.2f}%) is below threshold ({threshold}%).\")\n",
    "            logger.info(\"Initiating automated data cleaning...\")\n",
    "            \n",
    "            # Clean the data\n",
    "            cleaned_df, cleaning_stats = clean_data(df, validation_result)\n",
    "            return cleaned_df, True, cleaning_stats\n",
    "        else:\n",
    "            logger.info(f\"Data quality score ({quality_score:.2f}%) is above threshold ({threshold}%).\")\n",
    "            logger.info(\"No cleaning needed.\")\n",
    "            return df, False, cleaning_stats\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error checking quality and cleaning: {str(e)}\")\n",
    "        raise DataQualityError(f\"Failed to check quality and clean data: {str(e)}\")\n",
    "\n",
    "# Function to save cleaned data\n",
    "def save_cleaned_data(df: pd.DataFrame, file_path: str = 'cleaned_data.csv') -> bool:\n",
    "    \"\"\"\n",
    "    Save cleaned data to CSV with error handling\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to save\n",
    "        file_path: Path to save the cleaned data\n",
    "        \n",
    "    Returns:\n",
    "        True if saved successfully, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df.to_csv(file_path, index=False)\n",
    "        logger.info(f\"Cleaned data saved to {file_path}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving cleaned data: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Main function to execute the full data quality workflow\n",
    "def main(data_file: str = 'sample_data.csv', \n",
    "        output_dir: str = '.', \n",
    "        quality_threshold: float = 80.0,\n",
    "        validate_only: bool = False):\n",
    "    \"\"\"\n",
    "    Execute the full data quality workflow with better error handling and modularity\n",
    "    \n",
    "    Args:\n",
    "        data_file: Path to the input CSV file\n",
    "        output_dir: Directory for output files\n",
    "        quality_threshold: Threshold for triggering data cleaning\n",
    "        validate_only: If True, only validate without cleaning\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(\"Starting data quality workflow\")\n",
    "        \n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Step 1: Load or create data\n",
    "        logger.info(\"Loading data...\")\n",
    "        df = load_or_create_data(data_file)\n",
    "        logger.info(f\"Loaded data with {len(df)} rows and {len(df.columns)} columns\")\n",
    "        \n",
    "        # Step 2: Calculate basic metrics\n",
    "        logger.info(\"Calculating basic data quality metrics...\")\n",
    "        metrics = calculate_basic_metrics(df)\n",
    "        for column, completeness in metrics['completeness'].items():\n",
    "            logger.info(f\"Completeness ({column}): {completeness:.2f}%\")\n",
    "        logger.info(f\"Email Validity: {metrics['email_validity']:.2f}%\")\n",
    "        logger.info(f\"Email Uniqueness: {metrics['email_uniqueness']:.2f}%\")\n",
    "        \n",
    "        # Step 3: Calculate quality score\n",
    "        logger.info(\"Calculating overall data quality score...\")\n",
    "        score = calculate_quality_score(metrics)\n",
    "        logger.info(f\"Overall Data Quality Score: {score:.2f}%\")\n",
    "        \n",
    "        # Step 4: Create expectations\n",
    "        logger.info(\"Creating expectations using Great Expectations...\")\n",
    "        expectation_suite = create_expectations(\n",
    "            df, \n",
    "            file_path=data_file,\n",
    "            output_path=os.path.join(output_dir, 'expectation_suite.json')\n",
    "        )\n",
    "        logger.info(\"Expectations created successfully\")\n",
    "        \n",
    "        # Step 5: Validate data\n",
    "        logger.info(\"Validating data against expectations...\")\n",
    "        validation_result = validate_data(\n",
    "            df, \n",
    "            expectation_suite,\n",
    "            output_path=os.path.join(output_dir, 'validation_report.html')\n",
    "        )\n",
    "        \n",
    "        # Skip cleaning if validate_only is True\n",
    "        if validate_only:\n",
    "            logger.info(\"Validation complete. Skipping cleaning as validate_only=True\")\n",
    "            return\n",
    "        \n",
    "        # Step 6: Check quality and clean if needed\n",
    "        cleaned_df, was_cleaned, cleaning_stats = check_quality_and_clean(\n",
    "            df, score, validation_result, threshold=quality_threshold\n",
    "        )\n",
    "        \n",
    "        # Step 7: If data was cleaned, save it and recalculate metrics\n",
    "        if was_cleaned:\n",
    "            # Save the cleaned data\n",
    "            save_cleaned_data(cleaned_df, os.path.join(output_dir, 'cleaned_data.csv'))\n",
    "            \n",
    "            # Print cleaning statistics\n",
    "            logger.info(\"\\nCleaning Statistics:\")\n",
    "            logger.info(f\"Rows affected: {cleaning_stats['rows_affected']}\")\n",
    "            logger.info(f\"Cells cleaned: {cleaning_stats['cells_cleaned']}\")\n",
    "            logger.info(\"\\nCleaning actions performed:\")\n",
    "            for action in cleaning_stats['cleaning_actions']:\n",
    "                logger.info(f\"- {action}\")\n",
    "            \n",
    "            # Recalculate metrics and score for the cleaned data\n",
    "            logger.info(\"\\nRecalculating metrics after cleaning...\")\n",
    "            new_metrics = calculate_basic_metrics(cleaned_df)\n",
    "            new_score = calculate_quality_score(new_metrics)\n",
    "            \n",
    "            logger.info(f\"New Data Quality Score after cleaning: {new_score:.2f}%\")\n",
    "            \n",
    "            # Create new validation report for cleaned data\n",
    "            logger.info(\"Validating cleaned data...\")\n",
    "            validate_data(\n",
    "                cleaned_df, \n",
    "                expectation_suite,\n",
    "                output_path=os.path.join(output_dir, 'cleaned_validation_report.html')\n",
    "            )\n",
    "        \n",
    "        logger.info(\"Data quality workflow completed successfully\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in data quality workflow: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Unit tests for the data quality functions\n",
    "class TestDataQualityFunctions(unittest.TestCase):\n",
    "    \"\"\"Unit tests for the data quality functions\"\"\"\n",
    "    \n",
    "    def setUp(self):\n",
    "        \"\"\"Set up test data\"\"\"\n",
    "        self.test_data = pd.DataFrame({\n",
    "            'Name': ['Test User', None, 'Another User'],\n",
    "            'Email': ['test@example.com', 'invalid', 'another@example.com'],\n",
    "            'Age': [25, None, 'invalid']\n",
    "        })\n",
    "    \n",
    "    def test_calculate_basic_metrics(self):\n",
    "        \"\"\"Test calculate_basic_metrics function\"\"\"\n",
    "        metrics = calculate_basic_metrics(self.test_data)\n",
    "        \n",
    "        # Check completeness\n",
    "        self.assertAlmostEqual(metrics['completeness']['Name'], 2/3 * 100)\n",
    "        self.assertAlmostEqual(metrics['completeness']['Email'], 3/3 * 100)\n",
    "        self.assertAlmostEqual(metrics['completeness']['Age'], 2/3 * 100)\n",
    "        \n",
    "        # Check email validity\n",
    "        self.assertAlmostEqual(metrics['email_validity'], 2/3 * 100)\n",
    "        \n",
    "        # Check email uniqueness\n",
    "        self.assertAlmostEqual(metrics['email_uniqueness'], 3/3 * 100)\n",
    "    \n",
    "    def test_calculate_quality_score(self):\n",
    "        \"\"\"Test calculate_quality_score function\"\"\"\n",
    "        metrics = {\n",
    "            'completeness': {'Name': 80, 'Email': 90, 'Age': 70},\n",
    "            'email_validity': 60,\n",
    "            'email_uniqueness': 100\n",
    "        }\n",
    "        \n",
    "        score = calculate_quality_score(metrics)\n",
    "        expected_score = ((80 + 90 + 70) / 3 + 60 + 100) / 3\n",
    "        self.assertAlmostEqual(score, expected_score)\n",
    "    \n",
    "    def test_clean_data(self):\n",
    "        \"\"\"Test clean_data function\"\"\"\n",
    "        # Mock validation result\n",
    "        mock_expectation = MagicMock()\n",
    "        mock_expectation.expectation_type = 'expect_column_values_to_not_be_null'\n",
    "        mock_expectation.kwargs = {'column': 'Name'}\n",
    "        \n",
    "        mock_result = MagicMock()\n",
    "        mock_result.expectation_config = mock_expectation\n",
    "        mock_result.success = False\n",
    "        \n",
    "        mock_validation = MagicMock()\n",
    "        mock_validation.results = [mock_result]\n",
    "        \n",
    "        # Test cleaning\n",
    "        cleaned_df, stats = clean_data(self.test_data, mock_validation)\n",
    "        \n",
    "        # Check if missing name was filled\n",
    "        self.assertEqual(cleaned_df.loc[1, 'Name'], 'Unknown User')\n",
    "        \n",
    "        # Check if stats were recorded\n",
    "        self.assertEqual(stats['cells_cleaned'], 1)\n",
    "        self.assertEqual(stats['rows_affected'], 1)\n",
    "        self.assertEqual(len(stats['cleaning_actions']), 1)\n",
    "    \n",
    "    def test_empty_dataframe(self):\n",
    "        \"\"\"Test handling of empty DataFrame\"\"\"\n",
    "        empty_df = pd.DataFrame()\n",
    "        with self.assertRaises(DataQualityError):\n",
    "            calculate_basic_metrics(empty_df)\n",
    "    \n",
    "    def test_validate_data(self):\n",
    "        \"\"\"Test validate_data function\"\"\"\n",
    "        # Create a simple expectation suite\n",
    "        suite = ExpectationSuite(expectation_suite_name=\"test_suite\")\n",
    "        \n",
    "        # Test validation\n",
    "        with patch('builtins.open', MagicMock()):  # Mock file writing\n",
    "            result = validate_data(self.test_data, suite)\n",
    "            self.assertIsNotNone(result)\n",
    "    \n",
    "    def test_check_quality_and_clean(self):\n",
    "        \"\"\"Test check_quality_and_clean function\"\"\"\n",
    "        # Mock validation result\n",
    "        mock_validation = MagicMock()\n",
    "        \n",
    "        # Test when score is above threshold\n",
    "        df, was_cleaned, stats = check_quality_and_clean(\n",
    "            self.test_data, 90.0, mock_validation, threshold=80.0\n",
    "        )\n",
    "        self.assertFalse(was_cleaned)\n",
    "        \n",
    "        # Test when score is below threshold\n",
    "        with patch('__main__.clean_data', return_value=(self.test_data, {'rows_affected': 1, 'cells_cleaned': 2, 'cleaning_actions': ['test']})):\n",
    "            df, was_cleaned, stats = check_quality_and_clean(\n",
    "                self.test_data, 70.0, mock_validation, threshold=80.0\n",
    "            )\n",
    "            self.assertTrue(was_cleaned)\n",
    "\n",
    "\n",
    "# Function to run the full workflow with command-line arguments\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    \n",
    "    # Parse command line arguments\n",
    "    parser = argparse.ArgumentParser(description='Data Quality Workflow')\n",
    "    parser.add_argument('--data', type=str, default='sample_data.csv', help='Path to input data file')\n",
    "    parser.add_argument('--output', type=str, default='.', help='Directory for output files')\n",
    "    parser.add_argument('--threshold', type=float, default=80.0, help='Quality threshold for cleaning')\n",
    "    parser.add_argument('--validate-only', action='store_true', help='Only validate data without cleaning')\n",
    "    parser.add_argument('--test', action='store_true', help='Run unit tests')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Run tests if --test flag is provided\n",
    "    if args.test:\n",
    "        import unittest\n",
    "        unittest.main(argv=['first-arg-is-ignored'])\n",
    "    else:\n",
    "        # Run the main workflow\n",
    "        main(data_file=args.data, output_dir=args.output, \n",
    "             quality_threshold=args.threshold, validate_only=args.validate_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BaseDataContext' from 'great_expectations.data_context' (/home/vscode/.local/lib/python3.10/site-packages/great_expectations/data_context/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, List, Tuple, Any, Optional\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExpectationSuite\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_context\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BaseDataContext\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_context\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataContextConfig\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RuntimeBatchRequest\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'BaseDataContext' from 'great_expectations.data_context' (/home/vscode/.local/lib/python3.10/site-packages/great_expectations/data_context/__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "from great_expectations.core import ExpectationSuite\n",
    "from great_expectations.data_context import BaseDataContext\n",
    "from great_expectations.data_context.types.base import DataContextConfig\n",
    "from great_expectations.core.batch import RuntimeBatchRequest\n",
    "from great_expectations.checkpoint import SimpleCheckpoint\n",
    "from great_expectations.exceptions import DataContextError\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class AutomatedDataCleaner:\n",
    "    \"\"\"\n",
    "    A class to handle automated data cleaning based on Great Expectations validations.\n",
    "    Implements Task 6: Leveraging Data Quality Metrics for Automated Data Cleaning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        data_file: str,\n",
    "        context_root_dir: str = \"great_expectations\",\n",
    "        quality_threshold: float = 80.0,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the AutomatedDataCleaner\n",
    "        \n",
    "        Args:\n",
    "            data_file: Path to the CSV file to process\n",
    "            context_root_dir: Directory for Great Expectations context\n",
    "            quality_threshold: Quality threshold to trigger cleaning\n",
    "        \"\"\"\n",
    "        self.data_file = data_file\n",
    "        self.context_root_dir = context_root_dir\n",
    "        self.quality_threshold = quality_threshold\n",
    "        self.context = self._initialize_context()\n",
    "        \n",
    "    def _initialize_context(self) -> BaseDataContext:\n",
    "        \"\"\"\n",
    "        Create or load a Great Expectations context\n",
    "        \n",
    "        Returns:\n",
    "            Great Expectations context\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create directory if it doesn't exist\n",
    "            os.makedirs(self.context_root_dir, exist_ok=True)\n",
    "            \n",
    "            # Define a simple data context config\n",
    "            data_context_config = DataContextConfig(\n",
    "                store_backend_defaults={\"class_name\": \"InMemoryStoreBackend\"},\n",
    "                datasources={},\n",
    "                expectations_store_name=\"expectations_store\",\n",
    "                validations_store_name=\"validations_store\",\n",
    "                evaluation_parameter_store_name=\"evaluation_parameter_store\",\n",
    "                checkpoint_store_name=\"checkpoint_store\",\n",
    "                config_version=3.0,\n",
    "            )\n",
    "            \n",
    "            # Create or return the data context\n",
    "            context = BaseDataContext(project_config=data_context_config)\n",
    "            return context\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing Great Expectations context: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def create_expectation_suite(self, suite_name: str = \"data_quality_suite\") -> ExpectationSuite:\n",
    "        \"\"\"\n",
    "        Create an expectation suite with standard data quality expectations\n",
    "        \n",
    "        Args:\n",
    "            suite_name: Name for the expectation suite\n",
    "            \n",
    "        Returns:\n",
    "            ExpectationSuite object\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create a new expectation suite\n",
    "            suite = self.context.create_expectation_suite(\n",
    "                expectation_suite_name=suite_name,\n",
    "                overwrite_existing=True\n",
    "            )\n",
    "            \n",
    "            # Load the data to infer appropriate expectations\n",
    "            df = pd.read_csv(self.data_file)\n",
    "            \n",
    "            # Create a batch from the dataframe\n",
    "            batch = self.context.get_batch(\n",
    "                batch_request=RuntimeBatchRequest(\n",
    "                    datasource_name=\"my_pandas_datasource\",\n",
    "                    data_connector_name=\"default_runtime_data_connector_name\",\n",
    "                    data_asset_name=\"my_data_asset\",\n",
    "                    runtime_parameters={\"batch_data\": df},\n",
    "                    batch_identifiers={\"default_identifier_name\": \"default_identifier\"},\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Add general expectations for all columns\n",
    "            for column in df.columns:\n",
    "                # Completeness - expect not null\n",
    "                batch.expect_column_values_to_not_be_null(column, mostly=0.8)\n",
    "                \n",
    "                # Type consistency\n",
    "                if pd.api.types.is_numeric_dtype(df[column]):\n",
    "                    batch.expect_column_values_to_be_in_type_list(\n",
    "                        column, [\"INTEGER\", \"FLOAT\", \"DOUBLE\", \"DECIMAL\", \"NUMERIC\"]\n",
    "                    )\n",
    "            \n",
    "            # Add specific expectations for email columns\n",
    "            if 'email' in df.columns.str.lower().tolist() or 'Email' in df.columns:\n",
    "                email_col = 'email' if 'email' in df.columns else 'Email'\n",
    "                batch.expect_column_values_to_match_regex(\n",
    "                    email_col, r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$', mostly=0.9\n",
    "                )\n",
    "                batch.expect_column_values_to_be_unique(email_col)\n",
    "            \n",
    "            # Add specific expectations for name columns\n",
    "            if 'name' in df.columns.str.lower().tolist() or 'Name' in df.columns:\n",
    "                name_col = 'name' if 'name' in df.columns else 'Name'\n",
    "                batch.expect_column_values_to_not_be_null(name_col, mostly=0.9)\n",
    "                batch.expect_column_value_lengths_to_be_between(name_col, min_value=2)\n",
    "            \n",
    "            # Add specific expectations for age columns\n",
    "            if 'age' in df.columns.str.lower().tolist() or 'Age' in df.columns:\n",
    "                age_col = 'age' if 'age' in df.columns else 'Age'\n",
    "                batch.expect_column_values_to_be_between(age_col, min_value=0, max_value=120)\n",
    "            \n",
    "            # Save the expectation suite\n",
    "            self.context.save_expectation_suite(batch.get_expectation_suite(), suite_name)\n",
    "            logger.info(f\"Created expectation suite: {suite_name}\")\n",
    "            \n",
    "            return batch.get_expectation_suite()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating expectation suite: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def setup_cleaning_checkpoint(self, \n",
    "                                 suite_name: str = \"data_quality_suite\",\n",
    "                                 checkpoint_name: str = \"data_cleaning_checkpoint\") -> None:\n",
    "        \"\"\"\n",
    "        Set up a checkpoint that will trigger cleaning when validation fails\n",
    "        \n",
    "        Args:\n",
    "            suite_name: Name of the expectation suite to use\n",
    "            checkpoint_name: Name for the new checkpoint\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Configure a simple checkpoint with an action list\n",
    "            checkpoint_config = {\n",
    "                \"name\": checkpoint_name,\n",
    "                \"config_version\": 1.0,\n",
    "                \"class_name\": \"SimpleCheckpoint\",\n",
    "                \"run_name_template\": \"%Y%m%d-%H%M%S-cleaning-checkpoint\",\n",
    "                \"validations\": [\n",
    "                    {\n",
    "                        \"batch_request\": {\n",
    "                            \"datasource_name\": \"my_pandas_datasource\",\n",
    "                            \"data_connector_name\": \"default_runtime_data_connector_name\",\n",
    "                            \"data_asset_name\": \"my_data_asset\",\n",
    "                        },\n",
    "                        \"expectation_suite_name\": suite_name\n",
    "                    }\n",
    "                ],\n",
    "                \"action_list\": [\n",
    "                    {\n",
    "                        \"name\": \"store_validation_result\",\n",
    "                        \"action\": {\"class_name\": \"StoreValidationResultAction\"}\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"store_evaluation_params\",\n",
    "                        \"action\": {\"class_name\": \"StoreEvaluationParametersAction\"}\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"update_data_docs\",\n",
    "                        \"action\": {\"class_name\": \"UpdateDataDocsAction\"}\n",
    "                    },\n",
    "                    {\n",
    "                        \"name\": \"clean_data_on_failure\",\n",
    "                        \"action\": {\n",
    "                            \"class_name\": \"CustomAction\",\n",
    "                            \"module_name\": \"custom_actions\",\n",
    "                            \"function_name\": \"clean_data_on_failure\",\n",
    "                            \"kwargs\": {\n",
    "                                \"threshold\": self.quality_threshold,\n",
    "                                \"output_path\": \"cleaned_data.csv\"\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "            \n",
    "            # Register the checkpoint\n",
    "            self.context.add_checkpoint(**checkpoint_config)\n",
    "            logger.info(f\"Created checkpoint: {checkpoint_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error setting up cleaning checkpoint: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    def calculate_quality_score(self, validation_result: dict) -> float:\n",
    "        \"\"\"\n",
    "        Calculate a quality score from validation results\n",
    "        \n",
    "        Args:\n",
    "            validation_result: Great Expectations validation result\n",
    "            \n",
    "        Returns:\n",
    "            Quality score as a percentage\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not validation_result or \"results\" not in validation_result:\n",
    "                return 0.0\n",
    "                \n",
    "            expectations_count = len(validation_result[\"results\"])\n",
    "            if expectations_count == 0:\n",
    "                return 0.0\n",
    "                \n",
    "            passed_count = sum(1 for r in validation_result[\"results\"] if r.get(\"success\", False))\n",
    "            quality_score = (passed_count / expectations_count) * 100\n",
    "            \n",
    "            return quality_score\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calculating quality score: {str(e)}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def clean_data(self, df: pd.DataFrame, validation_result: dict) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Clean data based on validation results with vectorized operations\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to clean\n",
    "            validation_result: Validation result dictionary\n",
    "            \n",
    "        Returns:\n",
    "            Cleaned DataFrame\n",
    "        \"\"\"\n",
    "        cleaned_df = df.copy()\n",
    "        \n",
    "        try:\n",
    "            # Extract failed expectations\n",
    "            failed_expectations = []\n",
    "            for result in validation_result.get(\"results\", []):\n",
    "                if not result.get(\"success\", True):\n",
    "                    failed_expectations.append(result.get(\"expectation_config\", {}))\n",
    "            \n",
    "            # Process each failed expectation and apply appropriate cleaning\n",
    "            for expectation in failed_expectations:\n",
    "                expectation_type = expectation.get(\"expectation_type\", \"\")\n",
    "                column = expectation.get(\"kwargs\", {}).get(\"column\")\n",
    "                \n",
    "                if not column or column not in cleaned_df.columns:\n",
    "                    continue\n",
    "                \n",
    "                # Handle missing values\n",
    "                if expectation_type == \"expect_column_values_to_not_be_null\":\n",
    "                    self._clean_missing_values(cleaned_df, column)\n",
    "                \n",
    "                # Handle type mismatches\n",
    "                elif expectation_type in [\"expect_column_values_to_be_of_type\", \n",
    "                                         \"expect_column_values_to_be_in_type_list\"]:\n",
    "                    self._clean_type_issues(cleaned_df, column, expectation)\n",
    "                \n",
    "                # Handle regex pattern mismatches (like email format)\n",
    "                elif expectation_type == \"expect_column_values_to_match_regex\":\n",
    "                    pattern = expectation.get(\"kwargs\", {}).get(\"regex\")\n",
    "                    if pattern and \"email\" in column.lower():\n",
    "                        self._clean_email_format(cleaned_df, column, pattern)\n",
    "                \n",
    "                # Handle value range issues\n",
    "                elif expectation_type == \"expect_column_values_to_be_between\":\n",
    "                    min_val = expectation.get(\"kwargs\", {}).get(\"min_value\")\n",
    "                    max_val = expectation.get(\"kwargs\", {}).get(\"max_value\")\n",
    "                    self._clean_value_ranges(cleaned_df, column, min_val, max_val)\n",
    "                \n",
    "                # Handle uniqueness issues\n",
    "                elif expectation_type == \"expect_column_values_to_be_unique\":\n",
    "                    self._clean_duplicate_values(cleaned_df, column)\n",
    "            \n",
    "            return cleaned_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error cleaning data: {str(e)}\")\n",
    "            # Return the original dataframe if cleaning fails\n",
    "            return df\n",
    "    \n",
    "    def _clean_missing_values(self, df: pd.DataFrame, column: str) -> None:\n",
    "        \"\"\"\n",
    "        Clean missing values in a column with appropriate strategies\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to modify in-place\n",
    "            column: Column to clean\n",
    "        \"\"\"\n",
    "        missing_mask = df[column].isna()\n",
    "        if not missing_mask.any():\n",
    "            return\n",
    "            \n",
    "        # Different strategies based on column type\n",
    "        if \"name\" in column.lower():\n",
    "            # For name columns, use a placeholder\n",
    "            df.loc[missing_mask, column] = \"Unknown\"\n",
    "            \n",
    "        elif \"email\" in column.lower():\n",
    "            # For email columns, generate from index\n",
    "            for idx in df[missing_mask].index:\n",
    "                df.loc[idx, column] = f\"user{idx}@example.com\"\n",
    "                \n",
    "        elif df[column].dtype.kind in 'bifc':  # numeric types\n",
    "            # For numeric columns, use median\n",
    "            median_val = df[column].median()\n",
    "            if pd.isna(median_val):\n",
    "                median_val = 0\n",
    "            df.loc[missing_mask, column] = median_val\n",
    "            \n",
    "        elif df[column].dtype == 'datetime64[ns]':\n",
    "            # For datetime columns, use current date\n",
    "            df.loc[missing_mask, column] = pd.Timestamp.now()\n",
    "            \n",
    "        else:\n",
    "            # For other types, use a generic placeholder\n",
    "            df.loc[missing_mask, column] = \"Unknown\"\n",
    "    \n",
    "    def _clean_type_issues(self, df: pd.DataFrame, column: str, expectation: dict) -> None:\n",
    "        \"\"\"\n",
    "        Clean type issues in a column\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to modify in-place\n",
    "            column: Column to clean\n",
    "            expectation: The failed expectation details\n",
    "        \"\"\"\n",
    "        expected_type = None\n",
    "        if \"type_list\" in expectation.get(\"kwargs\", {}):\n",
    "            expected_types = expectation[\"kwargs\"][\"type_list\"]\n",
    "            if expected_types:\n",
    "                expected_type = expected_types[0]\n",
    "        elif \"type_\" in expectation.get(\"kwargs\", {}):\n",
    "            expected_type = expectation[\"kwargs\"][\"type_\"]\n",
    "        \n",
    "        # Handle numeric conversions\n",
    "        if expected_type in [\"INTEGER\", \"FLOAT\", \"DOUBLE\", \"DECIMAL\", \"NUMERIC\", \"int\", \"float\"]:\n",
    "            # Convert to numeric, coercing errors to NaN\n",
    "            df[column] = pd.to_numeric(df[column], errors='coerce')\n",
    "            \n",
    "            # Fill NaN values with appropriate defaults\n",
    "            missing_mask = df[column].isna()\n",
    "            if missing_mask.any():\n",
    "                # Use median or 0\n",
    "                median_val = df[column].median()\n",
    "                if pd.isna(median_val):\n",
    "                    df.loc[missing_mask, column] = 0\n",
    "                else:\n",
    "                    df.loc[missing_mask, column] = median_val\n",
    "        \n",
    "        # Handle string conversions\n",
    "        elif expected_type in [\"STRING\", \"TEXT\", \"VARCHAR\", \"str\"]:\n",
    "            # Convert to string\n",
    "            non_null_mask = ~df[column].isna()\n",
    "            df.loc[non_null_mask, column] = df.loc[non_null_mask, column].astype(str)\n",
    "    \n",
    "    def _clean_email_format(self, df: pd.DataFrame, column: str, pattern: str) -> None:\n",
    "        \"\"\"\n",
    "        Clean email format issues\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to modify in-place\n",
    "            column: Column to clean\n",
    "            pattern: Regex pattern for valid emails\n",
    "        \"\"\"\n",
    "        import re\n",
    "        \n",
    "        # Find invalid emails\n",
    "        non_null_mask = ~df[column].isna()\n",
    "        valid_pattern = re.compile(pattern)\n",
    "        \n",
    "        # Check each value against the pattern\n",
    "        for idx in df[non_null_mask].index:\n",
    "            email = str(df.loc[idx, column])\n",
    "            if not valid_pattern.match(email):\n",
    "                # Generate a replacement email\n",
    "                if 'name' in df.columns:\n",
    "                    name = df.loc[idx, 'name']\n",
    "                    if pd.notna(name):\n",
    "                        name_parts = str(name).lower().replace(' ', '.').replace('-', '.')\n",
    "                        df.loc[idx, column] = f\"{name_parts}@example.com\"\n",
    "                    else:\n",
    "                        df.loc[idx, column] = f\"user{idx}@example.com\"\n",
    "                else:\n",
    "                    df.loc[idx, column] = f\"user{idx}@example.com\"\n",
    "    \n",
    "    def _clean_value_ranges(self, df: pd.DataFrame, column: str, min_val: Any, max_val: Any) -> None:\n",
    "        \"\"\"\n",
    "        Clean values outside of valid ranges\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to modify in-place\n",
    "            column: Column to clean\n",
    "            min_val: Minimum allowed value\n",
    "            max_val: Maximum allowed value\n",
    "        \"\"\"\n",
    "        if pd.api.types.is_numeric_dtype(df[column]):\n",
    "            # For numeric columns, clip to the valid range\n",
    "            if min_val is not None and max_val is not None:\n",
    "                df[column] = df[column].clip(min_val, max_val)\n",
    "            elif min_val is not None:\n",
    "                df[column] = df[column].clip(lower=min_val)\n",
    "            elif max_val is not None:\n",
    "                df[column] = df[column].clip(upper=max_val)\n",
    "    \n",
    "    def _clean_duplicate_values(self, df: pd.DataFrame, column: str) -> None:\n",
    "        \"\"\"\n",
    "        Clean duplicate values in a column\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to modify in-place\n",
    "            column: Column to clean\n",
    "        \"\"\"\n",
    "        # Get duplicate values\n",
    "        duplicates = df[df.duplicated(subset=[column], keep='first')][column]\n",
    "        \n",
    "        if not duplicates.empty:\n",
    "            for idx in df[df.duplicated(subset=[column], keep='first')].index:\n",
    "                original_value = df.loc[idx, column]\n",
    "                \n",
    "                if pd.isna(original_value):\n",
    "                    # For NaN values, generate a unique placeholder\n",
    "                    df.loc[idx, column] = f\"unique_value_{idx}\"\n",
    "                elif isinstance(original_value, str):\n",
    "                    # For string values (like emails), add a suffix\n",
    "                    if '@' in original_value:\n",
    "                        # Handle email specifically\n",
    "                        username, domain = original_value.split('@', 1)\n",
    "                        df.loc[idx, column] = f\"{username}+{idx}@{domain}\"\n",
    "                    else:\n",
    "                        # Add a suffix\n",
    "                        df.loc[idx, column] = f\"{original_value}_{idx}\"\n",
    "                else:\n",
    "                    # For numeric or other types, add a small increment\n",
    "                    try:\n",
    "                        df.loc[idx, column] = original_value + (idx / 1000)\n",
    "                    except:\n",
    "                        # If incrementing fails, convert to string and add suffix\n",
    "                        df.loc[idx, column] = f\"{original_value}_{idx}\"\n",
    "    \n",
    "    def run_validation_and_cleaning(self, \n",
    "                                   suite_name: str = \"data_quality_suite\",\n",
    "                                   output_file: str = \"cleaned_data.csv\") -> Tuple[bool, float, Optional[str]]:\n",
    "        \"\"\"\n",
    "        Run validation and trigger cleaning if quality is below threshold\n",
    "        \n",
    "        Args:\n",
    "            suite_name: Name of the expectation suite to use\n",
    "            output_file: Path to save cleaned data if needed\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (was_cleaned, quality_score, cleaned_file_path)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load the data\n",
    "            df = pd.read_csv(self.data_file)\n",
    "            logger.info(f\"Loaded data from {self.data_file} with shape {df.shape}\")\n",
    "            \n",
    "            # Get or create the expectation suite\n",
    "            try:\n",
    "                suite = self.context.get_expectation_suite(suite_name)\n",
    "                logger.info(f\"Using existing expectation suite: {suite_name}\")\n",
    "            except DataContextError:\n",
    "                suite = self.create_expectation_suite(suite_name)\n",
    "                logger.info(f\"Created new expectation suite: {suite_name}\")\n",
    "            \n",
    "            # Create a batch from the dataframe\n",
    "            batch = self.context.get_batch(\n",
    "                batch_request=RuntimeBatchRequest(\n",
    "                    datasource_name=\"my_pandas_datasource\",\n",
    "                    data_connector_name=\"default_runtime_data_connector_name\",\n",
    "                    data_asset_name=\"my_data_asset\",\n",
    "                    runtime_parameters={\"batch_data\": df},\n",
    "                    batch_identifiers={\"default_identifier_name\": \"default_identifier\"},\n",
    "                    batch_spec_passthrough={\"reader_method\": \"pandas\"},\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Validate the data\n",
    "            validation_result = batch.validate(expectation_suite=suite)\n",
    "            \n",
    "            # Calculate quality score\n",
    "            quality_score = self.calculate_quality_score(validation_result)\n",
    "            logger.info(f\"Data quality score: {quality_score:.2f}%\")\n",
    "            \n",
    "            # Decide if cleaning is needed\n",
    "            if quality_score < self.quality_threshold:\n",
    "                logger.info(f\"Quality score {quality_score:.2f}% is below threshold {self.quality_threshold}%. Cleaning data...\")\n",
    "                \n",
    "                # Clean the data\n",
    "                cleaned_df = self.clean_data(df, validation_result)\n",
    "                \n",
    "                # Save the cleaned data\n",
    "                cleaned_df.to_csv(output_file, index=False)\n",
    "                logger.info(f\"Cleaned data saved to {output_file}\")\n",
    "                \n",
    "                # Recalculate quality score after cleaning\n",
    "                cleaned_batch = self.context.get_batch(\n",
    "                    batch_request=RuntimeBatchRequest(\n",
    "                        datasource_name=\"my_pandas_datasource\",\n",
    "                        data_connector_name=\"default_runtime_data_connector_name\",\n",
    "                        data_asset_name=\"cleaned_data_asset\",\n",
    "                        runtime_parameters={\"batch_data\": cleaned_df},\n",
    "                        batch_identifiers={\"default_identifier_name\": \"default_identifier\"},\n",
    "                    )\n",
    "                )\n",
    "                \n",
    "                new_validation_result = cleaned_batch.validate(expectation_suite=suite)\n",
    "                new_quality_score = self.calculate_quality_score(new_validation_result)\n",
    "                logger.info(f\"New quality score after cleaning: {new_quality_score:.2f}%\")\n",
    "                \n",
    "                return True, new_quality_score, output_file\n",
    "            else:\n",
    "                logger.info(f\"Quality score {quality_score:.2f}% is above threshold {self.quality_threshold}%. No cleaning needed.\")\n",
    "                return False, quality_score, None\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in validation and cleaning: {str(e)}\")\n",
    "            return False, 0.0, None\n",
    "\n",
    "\n",
    "# Custom action module for Great Expectations\n",
    "class custom_actions:\n",
    "    \"\"\"Custom actions for Great Expectations\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def clean_data_on_failure(context, validation_result, threshold=80.0, output_path=\"cleaned_data.csv\"):\n",
    "        \"\"\"\n",
    "        Custom action to clean data when validation fails\n",
    "        \n",
    "        Args:\n",
    "            context: Great Expectations context\n",
    "            validation_result: Validation result data\n",
    "            threshold: Quality threshold\n",
    "            output_path: Path to save cleaned data\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary with results of cleaning action\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Calculate quality score\n",
    "            if not validation_result or \"results\" not in validation_result:\n",
    "                return {\"success\": False, \"message\": \"Invalid validation result\"}\n",
    "                \n",
    "            expectations_count = len(validation_result[\"results\"])\n",
    "            if expectations_count == 0:\n",
    "                return {\"success\": False, \"message\": \"No expectations were validated\"}\n",
    "                \n",
    "            passed_count = sum(1 for r in validation_result[\"results\"] if r.get(\"success\", False))\n",
    "            quality_score = (passed_count / expectations_count) * 100\n",
    "            \n",
    "            # Check if cleaning is needed\n",
    "            if quality_score < threshold:\n",
    "                # Get data batch\n",
    "                batch_id = validation_result.get(\"meta\", {}).get(\"batch_spec\", {}).get(\"id\")\n",
    "                if not batch_id:\n",
    "                    return {\"success\": False, \"message\": \"Could not identify batch\"}\n",
    "                \n",
    "                # Implement cleaning logic here\n",
    "                cleaner = AutomatedDataCleaner(\n",
    "                    data_file=validation_result.get(\"meta\", {}).get(\"batch_spec\", {}).get(\"path\"),\n",
    "                    quality_threshold=threshold\n",
    "                )\n",
    "                df = pd.read_csv(validation_result.get(\"meta\", {}).get(\"batch_spec\", {}).get(\"path\"))\n",
    "                cleaned_df = cleaner.clean_data(df, validation_result)\n",
    "                cleaned_df.to_csv(output_path, index=False)\n",
    "                \n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"message\": f\"Data cleaned and saved to {output_path}\",\n",
    "                    \"quality_score_before\": quality_score,\n",
    "                    \"output_path\": output_path\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    \"success\": True,\n",
    "                    \"message\": f\"No cleaning needed. Quality score {quality_score:.2f}% is above threshold {threshold}%.\",\n",
    "                    \"quality_score\": quality_score\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            return {\"success\": False, \"message\": f\"Error in clean_data_on_failure: {str(e)}\"}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    \n",
    "    # Parse command line arguments\n",
    "    parser = argparse.ArgumentParser(description='Automated Data Cleaning with Great Expectations')\n",
    "    parser.add_argument('--data', type=str, required=True, help='Path to input data file')\n",
    "    parser.add_argument('--output', type=str, default='cleaned_data.csv', help='Path for cleaned data output')\n",
    "    parser.add_argument('--threshold', type=float, default=80.0, help='Quality threshold for cleaning')\n",
    "    parser.add_argument('--ge-dir', type=str, default='great_expectations', help='Great Expectations directory')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Create and run the automated cleaner\n",
    "    cleaner = AutomatedDataCleaner(\n",
    "        data_file=args.data,\n",
    "        context_root_dir=args.ge_dir,\n",
    "        quality_threshold=args.threshold\n",
    "    )\n",
    "    \n",
    "    was_cleaned, quality_score, cleaned_file = cleaner.run_validation_and_cleaning(\n",
    "        output_file=args.output\n",
    "    )\n",
    "    \n",
    "    if was_cleaned:\n",
    "        logger.info(f\"Data was cleaned to a quality score of {quality_score:.2f}% and saved to {cleaned_file}\")\n",
    "    else:\n",
    "        logger.info(f\"No cleaning was needed. Quality score: {quality_score:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
