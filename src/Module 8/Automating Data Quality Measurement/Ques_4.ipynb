{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Automated Data Profiling\n",
    "\n",
    "**Steps**:\n",
    "1. Using Pandas-Profiling\n",
    "    - Generate a profile report for an existing CSV file.\n",
    "    - Customize the profile report to include correlations.\n",
    "    - Profile a specific subset of columns.\n",
    "2. Using Great Expectations\n",
    "    - Create a basic expectation suite for your data.\n",
    "    - Validate data against an expectation suite.\n",
    "    - Add multiple expectations to a suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'great_expectations.dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mge\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExpectationSuite\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PandasDataset\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m IsolationForest\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'great_expectations.dataset'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import great_expectations as ge\n",
    "from great_expectations.core import ExpectationSuite\n",
    "from great_expectations.dataset import PandasDataset\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import logging\n",
    "import smtplib\n",
    "from email.mime.text import MimeText\n",
    "from email.mime.multipart import MimeMultipart\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('data_quality.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DataQualityMonitor:\n",
    "    \"\"\"\n",
    "    Comprehensive data quality monitoring system with automated profiling,\n",
    "    real-time monitoring, and AI-powered anomaly detection.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, alert_threshold: float = 0.95):\n",
    "        \"\"\"\n",
    "        Initialize the data quality monitor.\n",
    "        \n",
    "        Args:\n",
    "            alert_threshold: Threshold below which alerts are triggered\n",
    "        \"\"\"\n",
    "        self.alert_threshold = alert_threshold\n",
    "        self.anomaly_model = None\n",
    "        self.scaler = StandardScaler()\n",
    "        self.baseline_metrics = {}\n",
    "        \n",
    "    def check_accuracy(self, df: pd.DataFrame, column_name: str, \n",
    "                      expected_values: List[Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Vectorized accuracy check using pandas operations.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            column_name: Column to check\n",
    "            expected_values: List of expected values\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing accuracy metrics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if column_name not in df.columns:\n",
    "                raise ValueError(f\"Column '{column_name}' not found in DataFrame\")\n",
    "            \n",
    "            # Vectorized operation - much faster than loops\n",
    "            valid_mask = df[column_name].isin(expected_values)\n",
    "            accuracy_score = valid_mask.sum() / len(df)\n",
    "            \n",
    "            # Use Great Expectations for detailed validation\n",
    "            df_ge = PandasDataset(df)\n",
    "            ge_result = df_ge.expect_column_values_to_be_in_set(\n",
    "                column_name, expected_values\n",
    "            )\n",
    "            \n",
    "            result = {\n",
    "                'accuracy_score': accuracy_score,\n",
    "                'valid_count': valid_mask.sum(),\n",
    "                'total_count': len(df),\n",
    "                'invalid_values': df[~valid_mask][column_name].unique().tolist(),\n",
    "                'ge_result': ge_result\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"Accuracy check for {column_name}: {accuracy_score:.3f}\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in accuracy check: {str(e)}\")\n",
    "            return {'error': str(e), 'accuracy_score': 0.0}\n",
    "    \n",
    "    def check_completeness(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Vectorized completeness check for all columns.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing completeness metrics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Vectorized operations for missing data calculation\n",
    "            missing_counts = df.isnull().sum()\n",
    "            total_counts = len(df)\n",
    "            completeness_ratios = 1 - (missing_counts / total_counts)\n",
    "            \n",
    "            result = {\n",
    "                'missing_counts': missing_counts.to_dict(),\n",
    "                'completeness_ratios': completeness_ratios.to_dict(),\n",
    "                'overall_completeness': completeness_ratios.mean(),\n",
    "                'columns_with_missing': missing_counts[missing_counts > 0].index.tolist()\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"Overall completeness: {result['overall_completeness']:.3f}\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in completeness check: {str(e)}\")\n",
    "            return {'error': str(e), 'overall_completeness': 0.0}\n",
    "    \n",
    "    def check_consistency(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Enhanced consistency check including duplicates and data type consistency.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing consistency metrics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Check for duplicate rows\n",
    "            duplicate_count = df.duplicated().sum()\n",
    "            duplicate_percentage = (duplicate_count / len(df)) * 100\n",
    "            \n",
    "            # Check data type consistency\n",
    "            dtype_consistency = {}\n",
    "            for col in df.columns:\n",
    "                # Check if column has mixed data types\n",
    "                if df[col].dtype == 'object':\n",
    "                    unique_types = set(type(x).__name__ for x in df[col].dropna())\n",
    "                    dtype_consistency[col] = len(unique_types) == 1\n",
    "                else:\n",
    "                    dtype_consistency[col] = True\n",
    "            \n",
    "            result = {\n",
    "                'duplicate_count': duplicate_count,\n",
    "                'duplicate_percentage': duplicate_percentage,\n",
    "                'dtype_consistency': dtype_consistency,\n",
    "                'consistency_score': 1 - (duplicate_percentage / 100)\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"Consistency check - Duplicates: {duplicate_count}\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in consistency check: {str(e)}\")\n",
    "            return {'error': str(e), 'consistency_score': 0.0}\n",
    "    \n",
    "    def check_uniqueness(self, df: pd.DataFrame, column_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Enhanced uniqueness check with additional metrics.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            column_name: Column to check\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing uniqueness metrics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if column_name not in df.columns:\n",
    "                raise ValueError(f\"Column '{column_name}' not found in DataFrame\")\n",
    "            \n",
    "            unique_count = df[column_name].nunique()\n",
    "            total_count = len(df)\n",
    "            uniqueness_ratio = unique_count / total_count\n",
    "            \n",
    "            # Check for potential ID columns (high uniqueness)\n",
    "            is_potential_id = uniqueness_ratio > 0.9\n",
    "            \n",
    "            result = {\n",
    "                'unique_count': unique_count,\n",
    "                'total_count': total_count,\n",
    "                'uniqueness_ratio': uniqueness_ratio,\n",
    "                'is_potential_id': is_potential_id,\n",
    "                'duplicate_values': df[column_name].value_counts().head(10).to_dict()\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"Uniqueness check for {column_name}: {uniqueness_ratio:.3f}\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in uniqueness check: {str(e)}\")\n",
    "            return {'error': str(e), 'uniqueness_ratio': 0.0}\n",
    "    \n",
    "    def check_timeliness(self, df: pd.DataFrame, date_column: str) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Enhanced timeliness check with additional temporal metrics.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            date_column: Date column to check\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing timeliness metrics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if date_column not in df.columns:\n",
    "                raise ValueError(f\"Column '{date_column}' not found in DataFrame\")\n",
    "            \n",
    "            # Convert to datetime if not already\n",
    "            if not pd.api.types.is_datetime64_any_dtype(df[date_column]):\n",
    "                date_series = pd.to_datetime(df[date_column], errors='coerce')\n",
    "            else:\n",
    "                date_series = df[date_column]\n",
    "            \n",
    "            # Calculate temporal metrics\n",
    "            min_date = date_series.min()\n",
    "            max_date = date_series.max()\n",
    "            date_range_days = (max_date - min_date).days\n",
    "            \n",
    "            # Check for future dates\n",
    "            current_date = pd.Timestamp.now()\n",
    "            future_dates = (date_series > current_date).sum()\n",
    "            \n",
    "            # Check for reasonable date range (not too old or too future)\n",
    "            reasonable_min = pd.Timestamp('1900-01-01')\n",
    "            reasonable_max = pd.Timestamp('2030-12-31')\n",
    "            unreasonable_dates = ((date_series < reasonable_min) | \n",
    "                                (date_series > reasonable_max)).sum()\n",
    "            \n",
    "            result = {\n",
    "                'min_date': min_date,\n",
    "                'max_date': max_date,\n",
    "                'date_range_days': date_range_days,\n",
    "                'future_dates_count': future_dates,\n",
    "                'unreasonable_dates_count': unreasonable_dates,\n",
    "                'null_dates_count': date_series.isnull().sum(),\n",
    "                'timeliness_score': 1 - (unreasonable_dates + future_dates) / len(df)\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"Timeliness check for {date_column}: {result['timeliness_score']:.3f}\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in timeliness check: {str(e)}\")\n",
    "            return {'error': str(e), 'timeliness_score': 0.0}\n",
    "    \n",
    "    def generate_profile_report(self, df: pd.DataFrame, output_path: str = None) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate a comprehensive data profile report.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            output_path: Optional path to save the report\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing profile metrics\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Basic statistics\n",
    "            profile = {\n",
    "                'basic_info': {\n",
    "                    'shape': df.shape,\n",
    "                    'memory_usage': df.memory_usage(deep=True).sum(),\n",
    "                    'data_types': df.dtypes.to_dict()\n",
    "                },\n",
    "                'completeness': self.check_completeness(df),\n",
    "                'consistency': self.check_consistency(df),\n",
    "                'statistical_summary': df.describe(include='all').to_dict()\n",
    "            }\n",
    "            \n",
    "            # Add correlation matrix for numeric columns\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "            if len(numeric_cols) > 1:\n",
    "                profile['correlations'] = df[numeric_cols].corr().to_dict()\n",
    "            \n",
    "            # Save report if path provided\n",
    "            if output_path:\n",
    "                with open(output_path, 'w') as f:\n",
    "                    json.dump(profile, f, indent=2, default=str)\n",
    "                logger.info(f\"Profile report saved to {output_path}\")\n",
    "            \n",
    "            return profile\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating profile report: {str(e)}\")\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "    def setup_email_alerts(self, smtp_server: str, smtp_port: int, \n",
    "                          email: str, password: str, recipients: List[str]):\n",
    "        \"\"\"\n",
    "        Configure email alerts for data quality issues.\n",
    "        \n",
    "        Args:\n",
    "            smtp_server: SMTP server address\n",
    "            smtp_port: SMTP server port\n",
    "            email: Sender email address\n",
    "            password: Sender email password\n",
    "            recipients: List of recipient email addresses\n",
    "        \"\"\"\n",
    "        self.email_config = {\n",
    "            'smtp_server': smtp_server,\n",
    "            'smtp_port': smtp_port,\n",
    "            'email': email,\n",
    "            'password': password,\n",
    "            'recipients': recipients\n",
    "        }\n",
    "        logger.info(\"Email alerts configured successfully\")\n",
    "    \n",
    "    def send_alert(self, subject: str, message: str):\n",
    "        \"\"\"\n",
    "        Send email alert for data quality issues.\n",
    "        \n",
    "        Args:\n",
    "            subject: Email subject\n",
    "            message: Email message content\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not hasattr(self, 'email_config'):\n",
    "                logger.warning(\"Email configuration not set up\")\n",
    "                return\n",
    "            \n",
    "            msg = MimeMultipart()\n",
    "            msg['From'] = self.email_config['email']\n",
    "            msg['To'] = ', '.join(self.email_config['recipients'])\n",
    "            msg['Subject'] = subject\n",
    "            \n",
    "            msg.attach(MimeText(message, 'plain'))\n",
    "            \n",
    "            server = smtplib.SMTP(self.email_config['smtp_server'], \n",
    "                                self.email_config['smtp_port'])\n",
    "            server.starttls()\n",
    "            server.login(self.email_config['email'], self.email_config['password'])\n",
    "            text = msg.as_string()\n",
    "            server.sendmail(self.email_config['email'], \n",
    "                          self.email_config['recipients'], text)\n",
    "            server.quit()\n",
    "            \n",
    "            logger.info(f\"Alert sent: {subject}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to send email alert: {str(e)}\")\n",
    "    \n",
    "    def train_anomaly_detector(self, df: pd.DataFrame, \n",
    "                             contamination: float = 0.1) -> None:\n",
    "        \"\"\"\n",
    "        Train an Isolation Forest model for anomaly detection.\n",
    "        \n",
    "        Args:\n",
    "            df: Training DataFrame\n",
    "            contamination: Expected proportion of outliers\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Select numeric columns for anomaly detection\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "            \n",
    "            if len(numeric_cols) == 0:\n",
    "                raise ValueError(\"No numeric columns found for anomaly detection\")\n",
    "            \n",
    "            # Prepare data\n",
    "            X = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "            X_scaled = self.scaler.fit_transform(X)\n",
    "            \n",
    "            # Train Isolation Forest\n",
    "            self.anomaly_model = IsolationForest(\n",
    "                contamination=contamination,\n",
    "                random_state=42,\n",
    "                n_estimators=100\n",
    "            )\n",
    "            self.anomaly_model.fit(X_scaled)\n",
    "            \n",
    "            # Store baseline metrics\n",
    "            self.baseline_metrics = {\n",
    "                'mean': df[numeric_cols].mean().to_dict(),\n",
    "                'std': df[numeric_cols].std().to_dict(),\n",
    "                'median': df[numeric_cols].median().to_dict()\n",
    "            }\n",
    "            \n",
    "            logger.info(\"Anomaly detection model trained successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error training anomaly detector: {str(e)}\")\n",
    "    \n",
    "    def detect_anomalies(self, df: pd.DataFrame) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Detect anomalies in the data using the trained model.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to check for anomalies\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing anomaly detection results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.anomaly_model is None:\n",
    "                raise ValueError(\"Anomaly detection model not trained\")\n",
    "            \n",
    "            # Select numeric columns\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "            X = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "            X_scaled = self.scaler.transform(X)\n",
    "            \n",
    "            # Predict anomalies\n",
    "            anomaly_predictions = self.anomaly_model.predict(X_scaled)\n",
    "            anomaly_scores = self.anomaly_model.decision_function(X_scaled)\n",
    "            \n",
    "            # Calculate anomaly statistics\n",
    "            anomaly_mask = anomaly_predictions == -1\n",
    "            anomaly_count = anomaly_mask.sum()\n",
    "            anomaly_percentage = (anomaly_count / len(df)) * 100\n",
    "            \n",
    "            result = {\n",
    "                'anomaly_count': anomaly_count,\n",
    "                'anomaly_percentage': anomaly_percentage,\n",
    "                'anomaly_indices': df.index[anomaly_mask].tolist(),\n",
    "                'anomaly_scores': anomaly_scores.tolist(),\n",
    "                'threshold_exceeded': anomaly_percentage > (100 - self.alert_threshold * 100)\n",
    "            }\n",
    "            \n",
    "            # Trigger alert if threshold exceeded\n",
    "            if result['threshold_exceeded']:\n",
    "                self.send_alert(\n",
    "                    \"Data Quality Alert: High Anomaly Rate\",\n",
    "                    f\"Detected {anomaly_percentage:.2f}% anomalies in the data\"\n",
    "                )\n",
    "            \n",
    "            logger.info(f\"Anomaly detection completed: {anomaly_percentage:.2f}% anomalies\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in anomaly detection: {str(e)}\")\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "    def custom_outlier_detection(self, df: pd.DataFrame, \n",
    "                               column: str, method: str = 'iqr') -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Custom outlier detection using statistical methods.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            column: Column to check for outliers\n",
    "            method: Method to use ('iqr', 'zscore', 'modified_zscore')\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing outlier detection results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if column not in df.columns:\n",
    "                raise ValueError(f\"Column '{column}' not found in DataFrame\")\n",
    "            \n",
    "            if not pd.api.types.is_numeric_dtype(df[column]):\n",
    "                raise ValueError(f\"Column '{column}' is not numeric\")\n",
    "            \n",
    "            data = df[column].dropna()\n",
    "            \n",
    "            if method == 'iqr':\n",
    "                Q1 = data.quantile(0.25)\n",
    "                Q3 = data.quantile(0.75)\n",
    "                IQR = Q3 - Q1\n",
    "                lower_bound = Q1 - 1.5 * IQR\n",
    "                upper_bound = Q3 + 1.5 * IQR\n",
    "                outliers = (data < lower_bound) | (data > upper_bound)\n",
    "                \n",
    "            elif method == 'zscore':\n",
    "                z_scores = np.abs((data - data.mean()) / data.std())\n",
    "                outliers = z_scores > 3\n",
    "                \n",
    "            elif method == 'modified_zscore':\n",
    "                median = data.median()\n",
    "                mad = np.median(np.abs(data - median))\n",
    "                modified_z_scores = 0.6745 * (data - median) / mad\n",
    "                outliers = np.abs(modified_z_scores) > 3.5\n",
    "                \n",
    "            else:\n",
    "                raise ValueError(f\"Unknown method: {method}\")\n",
    "            \n",
    "            outlier_count = outliers.sum()\n",
    "            outlier_percentage = (outlier_count / len(data)) * 100\n",
    "            \n",
    "            result = {\n",
    "                'method': method,\n",
    "                'outlier_count': outlier_count,\n",
    "                'outlier_percentage': outlier_percentage,\n",
    "                'outlier_values': data[outliers].tolist(),\n",
    "                'outlier_indices': data[outliers].index.tolist()\n",
    "            }\n",
    "            \n",
    "            logger.info(f\"Outlier detection ({method}) for {column}: {outlier_percentage:.2f}%\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in custom outlier detection: {str(e)}\")\n",
    "            return {'error': str(e)}\n",
    "    \n",
    "    def comprehensive_quality_check(self, df: pd.DataFrame, \n",
    "                                  config: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Perform comprehensive data quality assessment.\n",
    "        \n",
    "        Args:\n",
    "            df: Input DataFrame\n",
    "            config: Configuration dictionary with check parameters\n",
    "            \n",
    "        Returns:\n",
    "            Comprehensive quality assessment results\n",
    "        \"\"\"\n",
    "        try:\n",
    "            results = {\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'data_shape': df.shape,\n",
    "                'checks': {}\n",
    "            }\n",
    "            \n",
    "            # Accuracy checks\n",
    "            if 'accuracy_checks' in config:\n",
    "                for check in config['accuracy_checks']:\n",
    "                    column = check['column']\n",
    "                    expected_values = check['expected_values']\n",
    "                    results['checks'][f'accuracy_{column}'] = self.check_accuracy(\n",
    "                        df, column, expected_values\n",
    "                    )\n",
    "            \n",
    "            # Completeness check\n",
    "            results['checks']['completeness'] = self.check_completeness(df)\n",
    "            \n",
    "            # Consistency check\n",
    "            results['checks']['consistency'] = self.check_consistency(df)\n",
    "            \n",
    "            # Uniqueness checks\n",
    "            if 'uniqueness_checks' in config:\n",
    "                for column in config['uniqueness_checks']:\n",
    "                    results['checks'][f'uniqueness_{column}'] = self.check_uniqueness(\n",
    "                        df, column\n",
    "                    )\n",
    "            \n",
    "            # Timeliness checks\n",
    "            if 'timeliness_checks' in config:\n",
    "                for column in config['timeliness_checks']:\n",
    "                    results['checks'][f'timeliness_{column}'] = self.check_timeliness(\n",
    "                        df, column\n",
    "                    )\n",
    "            \n",
    "            # Anomaly detection\n",
    "            if hasattr(self, 'anomaly_model') and self.anomaly_model is not None:\n",
    "                results['checks']['anomalies'] = self.detect_anomalies(df)\n",
    "            \n",
    "            # Custom outlier detection\n",
    "            if 'outlier_checks' in config:\n",
    "                for check in config['outlier_checks']:\n",
    "                    column = check['column']\n",
    "                    method = check.get('method', 'iqr')\n",
    "                    results['checks'][f'outliers_{column}_{method}'] = \\\n",
    "                        self.custom_outlier_detection(df, column, method)\n",
    "            \n",
    "            # Calculate overall quality score\n",
    "            scores = []\n",
    "            for check_name, check_result in results['checks'].items():\n",
    "                if 'error' not in check_result:\n",
    "                    if 'accuracy_score' in check_result:\n",
    "                        scores.append(check_result['accuracy_score'])\n",
    "                    elif 'overall_completeness' in check_result:\n",
    "                        scores.append(check_result['overall_completeness'])\n",
    "                    elif 'consistency_score' in check_result:\n",
    "                        scores.append(check_result['consistency_score'])\n",
    "                    elif 'timeliness_score' in check_result:\n",
    "                        scores.append(check_result['timeliness_score'])\n",
    "            \n",
    "            results['overall_quality_score'] = np.mean(scores) if scores else 0.0\n",
    "            \n",
    "            # Trigger alerts if quality is below threshold\n",
    "            if results['overall_quality_score'] < self.alert_threshold:\n",
    "                self.send_alert(\n",
    "                    \"Data Quality Alert: Quality Below Threshold\",\n",
    "                    f\"Overall quality score: {results['overall_quality_score']:.3f}\"\n",
    "                )\n",
    "            \n",
    "            logger.info(f\"Comprehensive quality check completed. Score: {results['overall_quality_score']:.3f}\")\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in comprehensive quality check: {str(e)}\")\n",
    "            return {'error': str(e)}\n",
    "\n",
    "\n",
    "# Example usage and testing\n",
    "if __name__ == \"__main__\":\n",
    "    # Create sample data with quality issues\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    sample_data = {\n",
    "        'ID': range(1, 1001),\n",
    "        'Age': np.random.randint(18, 80, 1000),\n",
    "        'Salary': np.random.normal(60000, 15000, 1000),\n",
    "        'Department': np.random.choice(['IT', 'HR', 'Finance', 'Marketing'], 1000),\n",
    "        'JoiningDate': pd.date_range('2020-01-01', '2023-12-31', periods=1000),\n",
    "        'Performance': np.random.choice(['Good', 'Average', 'Excellent'], 1000)\n",
    "    }\n",
    "    \n",
    "    # Introduce some quality issues\n",
    "    sample_df = pd.DataFrame(sample_data)\n",
    "    \n",
    "    # Add missing values\n",
    "    sample_df.loc[50:60, 'Salary'] = None\n",
    "    sample_df.loc[100:105, 'Department'] = None\n",
    "    \n",
    "    # Add duplicates\n",
    "    sample_df = pd.concat([sample_df, sample_df.iloc[:5]], ignore_index=True)\n",
    "    \n",
    "    # Add outliers\n",
    "    sample_df.loc[200:205, 'Salary'] = 200000\n",
    "    sample_df.loc[300:302, 'Age'] = 150\n",
    "    \n",
    "    # Initialize monitor\n",
    "    monitor = DataQualityMonitor(alert_threshold=0.9)\n",
    "    \n",
    "    # Generate profile report\n",
    "    profile = monitor.generate_profile_report(sample_df, 'data_profile.json')\n",
    "    print(\"Profile report generated\")\n",
    "    \n",
    "    # Train anomaly detector\n",
    "    monitor.train_anomaly_detector(sample_df)\n",
    "    \n",
    "    # Define quality check configuration\n",
    "    quality_config = {\n",
    "        'accuracy_checks': [\n",
    "            {\n",
    "                'column': 'Department',\n",
    "                'expected_values': ['IT', 'HR', 'Finance', 'Marketing']\n",
    "            },\n",
    "            {\n",
    "                'column': 'Performance',\n",
    "                'expected_values': ['Good', 'Average', 'Excellent']\n",
    "            }\n",
    "        ],\n",
    "        'uniqueness_checks': ['ID'],\n",
    "        'timeliness_checks': ['JoiningDate'],\n",
    "        'outlier_checks': [\n",
    "            {'column': 'Salary', 'method': 'iqr'},\n",
    "            {'column': 'Age', 'method': 'zscore'}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Perform comprehensive quality check\n",
    "    quality_results = monitor.comprehensive_quality_check(sample_df, quality_config)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nOverall Quality Score: {quality_results['overall_quality_score']:.3f}\")\n",
    "    print(f\"Data Shape: {quality_results['data_shape']}\")\n",
    "    \n",
    "    # Setup email alerts (example configuration)\n",
    "    # monitor.setup_email_alerts(\n",
    "    #     smtp_server='smtp.gmail.com',\n",
    "    #     smtp_port=587,\n",
    "    #     email='your_email@gmail.com',\n",
    "    #     password='your_password',\n",
    "    #     recipients=['recipient@example.com']\n",
    "    # )\n",
    "    \n",
    "    print(\"\\nData quality monitoring system initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Real-time Monitoring of Data Quality\n",
    "\n",
    "**Steps**:\n",
    "1. Setting up Alerts for Quality Drops\n",
    "    - Use the logging library to set up a basic alert on failed expectations.\n",
    "    - Implementing alerts using email notifications.\n",
    "    - Using a dashboard like Grafana for visual alerts.\n",
    "        - Note: Example assumes integration with a monitoring system\n",
    "        - Alert setup would involve creating a data source and alert rule in Grafana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Using AI for Data Quality Monitoring\n",
    "**Steps**:\n",
    "1. Basic AI Models for Monitoring\n",
    "    - Train a simple anomaly detection model using Isolation Forest.\n",
    "    - Use a simple custom function based AI logic for outlier detection.\n",
    "    - Creating a monitoring function that utilizes a pre-trained machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
