{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- ML Monitoring Pipeline Execution ----\n",
      "\n",
      "1️⃣ Checking Model Performance Drift:\n",
      "Prod Accuracy: 0.43, Baseline: 1.00\n",
      "Prod Precision: 0.43, Baseline: 1.00\n",
      "⚠️ Model performance drift detected.\n",
      "\n",
      "2️⃣ Checking Feature Distribution Drift:\n",
      "f1 KS test p-value: 0.0000\n",
      "⚠️ Drift detected in feature 'f1'\n",
      "f2 KS test p-value: 0.0000\n",
      "⚠️ Drift detected in feature 'f2'\n",
      "f3 KS test p-value: 0.0000\n",
      "⚠️ Drift detected in feature 'f3'\n",
      "\n",
      "3️⃣ Checking Anomalies in Predictions:\n",
      "Detected 4 anomalies.\n",
      "\n",
      "---- Running Unit Tests ----\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 4 anomalies.\n",
      "f1 KS test p-value: 0.0000\n",
      "⚠️ Drift detected in feature 'f1'\n",
      "f2 KS test p-value: 0.0000\n",
      "⚠️ Drift detected in feature 'f2'\n",
      "f3 KS test p-value: 0.0000\n",
      "⚠️ Drift detected in feature 'f3'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.508s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prod Accuracy: 0.43, Baseline: 1.00\n",
      "Prod Precision: 0.43, Baseline: 1.00\n",
      "⚠️ Model performance drift detected.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import ks_2samp\n",
    "import unittest\n",
    "\n",
    "# ---------------- Sample Data Simulation ----------------\n",
    "\n",
    "def generate_data(n_samples=100, drift=False, anomaly=False):\n",
    "    np.random.seed(42)\n",
    "    X = np.random.normal(loc=0.0, scale=1.0, size=(n_samples, 3))\n",
    "    y = (X[:, 0] + X[:, 1] > 0).astype(int)\n",
    "\n",
    "    if drift:\n",
    "        X += np.random.normal(loc=2.0, scale=0.5, size=X.shape)  # simulate drift\n",
    "\n",
    "    if anomaly:\n",
    "        X[:5] = X[:5] + np.random.normal(loc=10.0, scale=5.0, size=X[:5].shape)  # simulate anomalies\n",
    "\n",
    "    return pd.DataFrame(X, columns=['f1', 'f2', 'f3']), pd.Series(y, name=\"target\")\n",
    "\n",
    "# ------------------ Model Training ----------------------\n",
    "\n",
    "def train_model(X_train, y_train):\n",
    "    try:\n",
    "        model = RandomForestClassifier(random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Model training error: {e}\")\n",
    "        return None\n",
    "\n",
    "# ------------ 1. Model Performance Drift Monitoring ------------\n",
    "\n",
    "def monitor_model_performance(model, X_prod, y_prod, baseline_metrics, threshold=0.05):\n",
    "    try:\n",
    "        y_pred = model.predict(X_prod)\n",
    "        accuracy = accuracy_score(y_prod, y_pred)\n",
    "        precision = precision_score(y_prod, y_pred)\n",
    "\n",
    "        print(f\"Prod Accuracy: {accuracy:.2f}, Baseline: {baseline_metrics['accuracy']:.2f}\")\n",
    "        print(f\"Prod Precision: {precision:.2f}, Baseline: {baseline_metrics['precision']:.2f}\")\n",
    "\n",
    "        if abs(baseline_metrics['accuracy'] - accuracy) > threshold or \\\n",
    "           abs(baseline_metrics['precision'] - precision) > threshold:\n",
    "            print(\"⚠️ Model performance drift detected.\")\n",
    "            return False\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error monitoring model performance: {e}\")\n",
    "        return False\n",
    "\n",
    "# ----------- 2. Feature Distribution Drift Monitoring -----------\n",
    "\n",
    "def monitor_feature_drift(X_train, X_prod, p_value_threshold=0.05):\n",
    "    try:\n",
    "        drift_detected = False\n",
    "        for column in X_train.columns:\n",
    "            stat, p_value = ks_2samp(X_train[column], X_prod[column])\n",
    "            print(f\"{column} KS test p-value: {p_value:.4f}\")\n",
    "            if p_value < p_value_threshold:\n",
    "                print(f\"⚠️ Drift detected in feature '{column}'\")\n",
    "                drift_detected = True\n",
    "        return not drift_detected\n",
    "    except Exception as e:\n",
    "        print(f\"Error monitoring feature drift: {e}\")\n",
    "        return False\n",
    "\n",
    "# ----------- 3. Anomaly Detection in Predictions ----------------\n",
    "\n",
    "def detect_anomalies(X_prod, model, threshold=3.0):\n",
    "    try:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_prod)\n",
    "        z_scores = np.abs(X_scaled)\n",
    "        anomalies = (z_scores > threshold).any(axis=1)\n",
    "\n",
    "        print(f\"Detected {anomalies.sum()} anomalies.\")\n",
    "        return anomalies\n",
    "    except Exception as e:\n",
    "        print(f\"Error in anomaly detection: {e}\")\n",
    "        return pd.Series([False] * len(X_prod))\n",
    "\n",
    "\n",
    "# ---------------------- Unit Tests ----------------------\n",
    "\n",
    "class TestMonitoringPipeline(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.X_train, self.y_train = generate_data()\n",
    "        self.X_prod, self.y_prod = generate_data(drift=True, anomaly=True)\n",
    "        self.model = train_model(self.X_train, self.y_train)\n",
    "        self.baseline_metrics = {\n",
    "            'accuracy': accuracy_score(self.y_train, self.model.predict(self.X_train)),\n",
    "            'precision': precision_score(self.y_train, self.model.predict(self.X_train))\n",
    "        }\n",
    "\n",
    "    def test_performance_drift(self):\n",
    "        result = monitor_model_performance(self.model, self.X_prod, self.y_prod, self.baseline_metrics)\n",
    "        self.assertIsInstance(result, bool)\n",
    "\n",
    "    def test_feature_drift(self):\n",
    "        result = monitor_feature_drift(self.X_train, self.X_prod)\n",
    "        self.assertIsInstance(result, bool)\n",
    "\n",
    "    def test_anomaly_detection(self):\n",
    "        anomalies = detect_anomalies(self.X_prod, self.model)\n",
    "        self.assertEqual(len(anomalies), len(self.X_prod))\n",
    "        self.assertTrue(anomalies.dtype == bool)\n",
    "\n",
    "\n",
    "# ------------------------ Run Pipeline ------------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"---- ML Monitoring Pipeline Execution ----\")\n",
    "\n",
    "    X_train, y_train = generate_data()\n",
    "    X_prod, y_prod = generate_data(drift=True, anomaly=True)\n",
    "    model = train_model(X_train, y_train)\n",
    "\n",
    "    baseline_metrics = {\n",
    "        'accuracy': accuracy_score(y_train, model.predict(X_train)),\n",
    "        'precision': precision_score(y_train, model.predict(X_train))\n",
    "    }\n",
    "\n",
    "    print(\"\\n1️⃣ Checking Model Performance Drift:\")\n",
    "    monitor_model_performance(model, X_prod, y_prod, baseline_metrics)\n",
    "\n",
    "    print(\"\\n2️⃣ Checking Feature Distribution Drift:\")\n",
    "    monitor_feature_drift(X_train, X_prod)\n",
    "\n",
    "    print(\"\\n3️⃣ Checking Anomalies in Predictions:\")\n",
    "    anomalies = detect_anomalies(X_prod, model)\n",
    "\n",
    "    print(\"\\n---- Running Unit Tests ----\")\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
